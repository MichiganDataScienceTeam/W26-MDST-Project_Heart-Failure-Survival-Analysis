{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "287625eb-0bf6-413b-8fd0-23464b759279",
   "metadata": {},
   "source": [
    "# Supervised Learning and Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c8cffc-08f4-4382-9cad-a423434ec4c6",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this notebook, you'll learn about **Supervised Learning** - where we train machine learning models on labeled data (where we know the correct answers) to make predictions on new, unseen data.\n",
    "\n",
    "**Our Goal:** Predict whether a heart failure patient will have a death event based on clinical measurements.\n",
    "\n",
    "**Key Concepts You'll Learn:**\n",
    "- How to properly split data into training and testing sets\n",
    "- Multiple classification algorithms and when to use each\n",
    "- How to evaluate model performance with appropriate metrics\n",
    "- How to compare different models to find the best one\n",
    "\n",
    "**The Dataset:** 299 heart failure patients with 11 clinical features (age, ejection fraction, creatinine levels, etc.) and a target variable (DEATH_EVENT: 0 = survived, 1 = deceased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22735644-01c0-4b9f-97fa-b7602ebc48e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "df = pd.read_csv('./heart_failure_clinical_records_dataset.csv') #make sure to replace with your data directory\n",
    "df.drop(columns=['time'],inplace=True) #drop the time column which is not one of the informative features \n",
    "df_features = df.iloc[:,:-1]\n",
    "df_target = df['DEATH_EVENT']\n",
    "\n",
    "X = df_features\n",
    "y = df_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "801b18f1-a3c0-419c-8a3e-2fec2f0cc07e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>anaemia</th>\n",
       "      <th>creatinine_phosphokinase</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>ejection_fraction</th>\n",
       "      <th>high_blood_pressure</th>\n",
       "      <th>platelets</th>\n",
       "      <th>serum_creatinine</th>\n",
       "      <th>serum_sodium</th>\n",
       "      <th>sex</th>\n",
       "      <th>smoking</th>\n",
       "      <th>DEATH_EVENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75.0</td>\n",
       "      <td>0</td>\n",
       "      <td>582</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>265000.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>130</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>55.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7861</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>263358.03</td>\n",
       "      <td>1.1</td>\n",
       "      <td>136</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>65.0</td>\n",
       "      <td>0</td>\n",
       "      <td>146</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>162000.00</td>\n",
       "      <td>1.3</td>\n",
       "      <td>129</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1</td>\n",
       "      <td>111</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>210000.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>137</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>65.0</td>\n",
       "      <td>1</td>\n",
       "      <td>160</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>327000.00</td>\n",
       "      <td>2.7</td>\n",
       "      <td>116</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>155000.00</td>\n",
       "      <td>1.1</td>\n",
       "      <td>143</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>55.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1820</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>270000.00</td>\n",
       "      <td>1.2</td>\n",
       "      <td>139</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>45.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2060</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>742000.00</td>\n",
       "      <td>0.8</td>\n",
       "      <td>138</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>45.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2413</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>140000.00</td>\n",
       "      <td>1.4</td>\n",
       "      <td>140</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>196</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>395000.00</td>\n",
       "      <td>1.6</td>\n",
       "      <td>136</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>299 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      age  anaemia  creatinine_phosphokinase  diabetes  ejection_fraction  \\\n",
       "0    75.0        0                       582         0                 20   \n",
       "1    55.0        0                      7861         0                 38   \n",
       "2    65.0        0                       146         0                 20   \n",
       "3    50.0        1                       111         0                 20   \n",
       "4    65.0        1                       160         1                 20   \n",
       "..    ...      ...                       ...       ...                ...   \n",
       "294  62.0        0                        61         1                 38   \n",
       "295  55.0        0                      1820         0                 38   \n",
       "296  45.0        0                      2060         1                 60   \n",
       "297  45.0        0                      2413         0                 38   \n",
       "298  50.0        0                       196         0                 45   \n",
       "\n",
       "     high_blood_pressure  platelets  serum_creatinine  serum_sodium  sex  \\\n",
       "0                      1  265000.00               1.9           130    1   \n",
       "1                      0  263358.03               1.1           136    1   \n",
       "2                      0  162000.00               1.3           129    1   \n",
       "3                      0  210000.00               1.9           137    1   \n",
       "4                      0  327000.00               2.7           116    0   \n",
       "..                   ...        ...               ...           ...  ...   \n",
       "294                    1  155000.00               1.1           143    1   \n",
       "295                    0  270000.00               1.2           139    0   \n",
       "296                    0  742000.00               0.8           138    0   \n",
       "297                    0  140000.00               1.4           140    1   \n",
       "298                    0  395000.00               1.6           136    1   \n",
       "\n",
       "     smoking  DEATH_EVENT  \n",
       "0          0            1  \n",
       "1          0            1  \n",
       "2          1            1  \n",
       "3          0            1  \n",
       "4          0            1  \n",
       "..       ...          ...  \n",
       "294        1            0  \n",
       "295        0            0  \n",
       "296        0            0  \n",
       "297        1            0  \n",
       "298        1            0  \n",
       "\n",
       "[299 rows x 12 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b10ca066-df7e-4f98-a891-e329fbc4b8ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>anaemia</th>\n",
       "      <th>creatinine_phosphokinase</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>ejection_fraction</th>\n",
       "      <th>high_blood_pressure</th>\n",
       "      <th>platelets</th>\n",
       "      <th>serum_creatinine</th>\n",
       "      <th>serum_sodium</th>\n",
       "      <th>sex</th>\n",
       "      <th>smoking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75.0</td>\n",
       "      <td>0</td>\n",
       "      <td>582</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>265000.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>130</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>55.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7861</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>263358.03</td>\n",
       "      <td>1.1</td>\n",
       "      <td>136</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>65.0</td>\n",
       "      <td>0</td>\n",
       "      <td>146</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>162000.00</td>\n",
       "      <td>1.3</td>\n",
       "      <td>129</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1</td>\n",
       "      <td>111</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>210000.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>137</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>65.0</td>\n",
       "      <td>1</td>\n",
       "      <td>160</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>327000.00</td>\n",
       "      <td>2.7</td>\n",
       "      <td>116</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>155000.00</td>\n",
       "      <td>1.1</td>\n",
       "      <td>143</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>55.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1820</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>270000.00</td>\n",
       "      <td>1.2</td>\n",
       "      <td>139</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>45.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2060</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>742000.00</td>\n",
       "      <td>0.8</td>\n",
       "      <td>138</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>45.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2413</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>140000.00</td>\n",
       "      <td>1.4</td>\n",
       "      <td>140</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>196</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>395000.00</td>\n",
       "      <td>1.6</td>\n",
       "      <td>136</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>299 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      age  anaemia  creatinine_phosphokinase  diabetes  ejection_fraction  \\\n",
       "0    75.0        0                       582         0                 20   \n",
       "1    55.0        0                      7861         0                 38   \n",
       "2    65.0        0                       146         0                 20   \n",
       "3    50.0        1                       111         0                 20   \n",
       "4    65.0        1                       160         1                 20   \n",
       "..    ...      ...                       ...       ...                ...   \n",
       "294  62.0        0                        61         1                 38   \n",
       "295  55.0        0                      1820         0                 38   \n",
       "296  45.0        0                      2060         1                 60   \n",
       "297  45.0        0                      2413         0                 38   \n",
       "298  50.0        0                       196         0                 45   \n",
       "\n",
       "     high_blood_pressure  platelets  serum_creatinine  serum_sodium  sex  \\\n",
       "0                      1  265000.00               1.9           130    1   \n",
       "1                      0  263358.03               1.1           136    1   \n",
       "2                      0  162000.00               1.3           129    1   \n",
       "3                      0  210000.00               1.9           137    1   \n",
       "4                      0  327000.00               2.7           116    0   \n",
       "..                   ...        ...               ...           ...  ...   \n",
       "294                    1  155000.00               1.1           143    1   \n",
       "295                    0  270000.00               1.2           139    0   \n",
       "296                    0  742000.00               0.8           138    0   \n",
       "297                    0  140000.00               1.4           140    1   \n",
       "298                    0  395000.00               1.6           136    1   \n",
       "\n",
       "     smoking  \n",
       "0          0  \n",
       "1          0  \n",
       "2          1  \n",
       "3          0  \n",
       "4          0  \n",
       "..       ...  \n",
       "294        1  \n",
       "295        0  \n",
       "296        0  \n",
       "297        1  \n",
       "298        1  \n",
       "\n",
       "[299 rows x 11 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b042fd7c-7f80-48b0-bb2f-f4d1373b687f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1\n",
       "1      1\n",
       "2      1\n",
       "3      1\n",
       "4      1\n",
       "      ..\n",
       "294    0\n",
       "295    0\n",
       "296    0\n",
       "297    0\n",
       "298    0\n",
       "Name: DEATH_EVENT, Length: 299, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63280080-f770-4f9d-9535-d8ca904a1dbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DEATH_EVENT\n",
       "0    203\n",
       "1     96\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a715f5-8bfe-4244-9d66-01647fadfca9",
   "metadata": {},
   "source": [
    "# Create train and test sets\n",
    "\n",
    "We should divide the dataset to train and test splits. We train our ML algorithms on train dataset and evaluate their performance on test set\n",
    "\n",
    "\n",
    "## Why Split Data?\n",
    "\n",
    "A critical principle in machine learning is **avoiding data leakage**. If we train AND test on the same data, a model can simply memorize the training data instead of learning generalizable patterns. This leads to overly optimistic performance metrics.\n",
    "\n",
    "**Solution:** Split data into:\n",
    "- **Training set (70%):** Used to train the model\n",
    "- **Test set (30%):** Used to evaluate performance on unseen data\n",
    "\n",
    "**Stratification:** We use `stratify=y` to maintain the same class distribution (proportion of death events) in both train and test sets. This is important for imbalanced datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce3f2392-b7d4-4dd1-a497-223079899211",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mSignature:\u001b[39m\n",
      "train_test_split(\n",
      "    *arrays,\n",
      "    test_size=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    train_size=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    random_state=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m,\n",
      "    stratify=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      ")\n",
      "\u001b[31mDocstring:\u001b[39m\n",
      "Split arrays or matrices into random train and test subsets.\n",
      "\n",
      "Quick utility that wraps input validation,\n",
      "``next(ShuffleSplit().split(X, y))``, and application to input data\n",
      "into a single call for splitting (and optionally subsampling) data into a\n",
      "one-liner.\n",
      "\n",
      "Read more in the :ref:`User Guide <cross_validation>`.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "*arrays : sequence of indexables with same length / shape[0]\n",
      "    Allowed inputs are lists, numpy arrays, scipy-sparse\n",
      "    matrices or pandas dataframes.\n",
      "\n",
      "test_size : float or int, default=None\n",
      "    If float, should be between 0.0 and 1.0 and represent the proportion\n",
      "    of the dataset to include in the test split. If int, represents the\n",
      "    absolute number of test samples. If None, the value is set to the\n",
      "    complement of the train size. If ``train_size`` is also None, it will\n",
      "    be set to 0.25.\n",
      "\n",
      "train_size : float or int, default=None\n",
      "    If float, should be between 0.0 and 1.0 and represent the\n",
      "    proportion of the dataset to include in the train split. If\n",
      "    int, represents the absolute number of train samples. If None,\n",
      "    the value is automatically set to the complement of the test size.\n",
      "\n",
      "random_state : int, RandomState instance or None, default=None\n",
      "    Controls the shuffling applied to the data before applying the split.\n",
      "    Pass an int for reproducible output across multiple function calls.\n",
      "    See :term:`Glossary <random_state>`.\n",
      "\n",
      "shuffle : bool, default=True\n",
      "    Whether or not to shuffle the data before splitting. If shuffle=False\n",
      "    then stratify must be None.\n",
      "\n",
      "stratify : array-like, default=None\n",
      "    If not None, data is split in a stratified fashion, using this as\n",
      "    the class labels.\n",
      "    Read more in the :ref:`User Guide <stratification>`.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "splitting : list, length=2 * len(arrays)\n",
      "    List containing train-test split of inputs.\n",
      "\n",
      "    .. versionadded:: 0.16\n",
      "        If the input is sparse, the output will be a\n",
      "        ``scipy.sparse.csr_matrix``. Else, output type is the same as the\n",
      "        input type.\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> import numpy as np\n",
      ">>> from sklearn.model_selection import train_test_split\n",
      ">>> X, y = np.arange(10).reshape((5, 2)), range(5)\n",
      ">>> X\n",
      "array([[0, 1],\n",
      "       [2, 3],\n",
      "       [4, 5],\n",
      "       [6, 7],\n",
      "       [8, 9]])\n",
      ">>> list(y)\n",
      "[0, 1, 2, 3, 4]\n",
      "\n",
      ">>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "...     X, y, test_size=0.33, random_state=42)\n",
      "...\n",
      ">>> X_train\n",
      "array([[4, 5],\n",
      "       [0, 1],\n",
      "       [6, 7]])\n",
      ">>> y_train\n",
      "[2, 0, 3]\n",
      ">>> X_test\n",
      "array([[2, 3],\n",
      "       [8, 9]])\n",
      ">>> y_test\n",
      "[1, 4]\n",
      "\n",
      ">>> train_test_split(y, shuffle=False)\n",
      "[[0, 1, 2], [3, 4]]\n",
      "\n",
      ">>> from sklearn import datasets\n",
      ">>> iris = datasets.load_iris(as_frame=True)\n",
      ">>> X, y = iris['data'], iris['target']\n",
      ">>> X.head()\n",
      "    sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
      "0                5.1               3.5                1.4               0.2\n",
      "1                4.9               3.0                1.4               0.2\n",
      "2                4.7               3.2                1.3               0.2\n",
      "3                4.6               3.1                1.5               0.2\n",
      "4                5.0               3.6                1.4               0.2\n",
      ">>> y.head()\n",
      "0    0\n",
      "1    0\n",
      "2    0\n",
      "3    0\n",
      "4    0\n",
      "...\n",
      "\n",
      ">>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "... X, y, test_size=0.33, random_state=42)\n",
      "...\n",
      ">>> X_train.head()\n",
      "    sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
      "96                 5.7               2.9                4.2               1.3\n",
      "105                7.6               3.0                6.6               2.1\n",
      "66                 5.6               3.0                4.5               1.5\n",
      "0                  5.1               3.5                1.4               0.2\n",
      "122                7.7               2.8                6.7               2.0\n",
      ">>> y_train.head()\n",
      "96     1\n",
      "105    2\n",
      "66     1\n",
      "0      0\n",
      "122    2\n",
      "...\n",
      ">>> X_test.head()\n",
      "    sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
      "73                 6.1               2.8                4.7               1.2\n",
      "18                 5.7               3.8                1.7               0.3\n",
      "118                7.7               2.6                6.9               2.3\n",
      "78                 6.0               2.9                4.5               1.5\n",
      "76                 6.8               2.8                4.8               1.4\n",
      ">>> y_test.head()\n",
      "73     1\n",
      "18     0\n",
      "118    2\n",
      "78     1\n",
      "76     1\n",
      "...\n",
      "\u001b[31mFile:\u001b[39m      ~/P/W26-MDST-Project_Heart-Failure-Survival-Analysis/venv/lib/python3.11/site-packages/sklearn/model_selection/_split.py\n",
      "\u001b[31mType:\u001b[39m      function"
     ]
    }
   ],
   "source": [
    "train_test_split?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "910c1e2e-5529-4968-99f6-59648995536c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3,stratify=y,random_state=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "990856a4-c00e-468c-95bc-988e773bb0dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(209, 11)\n",
      "(209,)\n",
      "(90, 11)\n",
      "(90,)\n",
      "y train:  DEATH_EVENT\n",
      "0    0.679426\n",
      "1    0.320574\n",
      "Name: proportion, dtype: float64\n",
      "y test:  DEATH_EVENT\n",
      "0    0.677778\n",
      "1    0.322222\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "print('y train: ',y_train.value_counts(normalize=True))\n",
    "print('y test: ',y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b49a9b4-dec6-41ba-a0d7-7791b1743dc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>anaemia</th>\n",
       "      <th>creatinine_phosphokinase</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>ejection_fraction</th>\n",
       "      <th>high_blood_pressure</th>\n",
       "      <th>platelets</th>\n",
       "      <th>serum_creatinine</th>\n",
       "      <th>serum_sodium</th>\n",
       "      <th>sex</th>\n",
       "      <th>smoking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>58.0</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>153000.0</td>\n",
       "      <td>5.8</td>\n",
       "      <td>134</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1</td>\n",
       "      <td>582</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>279000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>134</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>60.0</td>\n",
       "      <td>1</td>\n",
       "      <td>737</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>210000.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>135</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>55.0</td>\n",
       "      <td>0</td>\n",
       "      <td>582</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>371000.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>140</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>72.0</td>\n",
       "      <td>1</td>\n",
       "      <td>328</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>621000.0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>138</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>85.0</td>\n",
       "      <td>1</td>\n",
       "      <td>910</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>235000.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>134</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>66.0</td>\n",
       "      <td>1</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>242000.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>134</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>59.0</td>\n",
       "      <td>1</td>\n",
       "      <td>280</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>302000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>141</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>45.0</td>\n",
       "      <td>0</td>\n",
       "      <td>582</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>302000.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>140</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>582</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>310000.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>135</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      age  anaemia  creatinine_phosphokinase  diabetes  ejection_fraction  \\\n",
       "28   58.0        1                        60         0                 38   \n",
       "198  50.0        1                       582         1                 20   \n",
       "120  60.0        1                       737         0                 60   \n",
       "219  55.0        0                       582         1                 35   \n",
       "105  72.0        1                       328         0                 30   \n",
       "..    ...      ...                       ...       ...                ...   \n",
       "158  85.0        1                       910         0                 50   \n",
       "160  66.0        1                        72         0                 40   \n",
       "84   59.0        1                       280         1                 25   \n",
       "268  45.0        0                       582         1                 38   \n",
       "45   50.0        0                       582         1                 38   \n",
       "\n",
       "     high_blood_pressure  platelets  serum_creatinine  serum_sodium  sex  \\\n",
       "28                     0   153000.0               5.8           134    1   \n",
       "198                    1   279000.0               1.0           134    0   \n",
       "120                    1   210000.0               1.5           135    1   \n",
       "219                    1   371000.0               0.7           140    0   \n",
       "105                    1   621000.0               1.7           138    0   \n",
       "..                   ...        ...               ...           ...  ...   \n",
       "158                    0   235000.0               1.3           134    1   \n",
       "160                    1   242000.0               1.2           134    1   \n",
       "84                     1   302000.0               1.0           141    0   \n",
       "268                    0   302000.0               0.9           140    0   \n",
       "45                     0   310000.0               1.9           135    1   \n",
       "\n",
       "     smoking  \n",
       "28         0  \n",
       "198        0  \n",
       "120        1  \n",
       "219        0  \n",
       "105        1  \n",
       "..       ...  \n",
       "158        0  \n",
       "160        0  \n",
       "84         0  \n",
       "268        0  \n",
       "45         1  \n",
       "\n",
       "[90 rows x 11 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c36c5e4-5fdd-4459-8f29-5d156d3d9193",
   "metadata": {},
   "source": [
    "A model with random guess which always predict the majority group has 68% accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091f5147-95ec-4e9c-8379-459c0741ec8a",
   "metadata": {},
   "source": [
    "# Normalize train and test sets separately\n",
    "\n",
    "\n",
    "## Why Normalize Data?\n",
    "\n",
    "Our features have different scales:\n",
    "- **Age:** 40-80 years\n",
    "- **Ejection Fraction:** 15-80 %\n",
    "- **Platelets:** 25,000-850,000 cells/μL\n",
    "\n",
    "Algorithms like SVM and KNN are **distance-based** - they calculate distances between data points. Large-scale features (like platelets) would dominate the distance calculation, making small-scale features (like ejection fraction) irrelevant.\n",
    "\n",
    "**Z-score Normalization:** Transforms each feature to have mean=0 and standard deviation=1. This puts all features on the same scale.\n",
    "\n",
    "**Important:** We normalize training and test sets separately to avoid data leakage (we shouldn't use test data statistics to transform the training data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb9e4644-9efd-43d1-a380-f6ccb729c8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "X_train = zscore(X_train)\n",
    "X_test = zscore(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66daf195-ddb0-48b9-9b0c-853a54a9621e",
   "metadata": {},
   "source": [
    "# Logistic regression Model \n",
    "\n",
    "1- Establish your model \n",
    "\n",
    "2- Fit your model \n",
    "\n",
    "3- Predict on test set\n",
    "\n",
    "4- Evaluation on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da6cf637-0b5f-44b6-9c63-f5ca8df2a13f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7111111111111111\n",
      "Precision: 0.5789473684210527\n",
      "Recall: 0.3793103448275862\n",
      "F1-Score: 0.4583333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "preds = model.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, preds))\n",
    "print(\"Precision:\", precision_score(y_test, preds))\n",
    "print(\"Recall:\", recall_score(y_test, preds))\n",
    "print(\"F1-Score:\", f1_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e013714c-3b80-4399-8daf-3cb4d1b0bc75",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "\n",
    "## Exercise: Train a Random Forest model on train set and report Accuracy, Precision, Recall, F1-Score on test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1958cb0a-4d74-47d3-9add-489546430563",
   "metadata": {},
   "outputs": [],
   "source": "from sklearn.ensemble import RandomForestClassifier\n\nmodel = RandomForestClassifier()\nmodel.fit(X_train, y_train)\n\npreds = model.predict(X_test)\npreds"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93df4a8-847a-4779-99a0-49549fecf1fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28     1\n",
       "198    0\n",
       "120    0\n",
       "219    0\n",
       "105    1\n",
       "62     0\n",
       "178    0\n",
       "75     1\n",
       "211    0\n",
       "276    0\n",
       "264    0\n",
       "11     1\n",
       "19     1\n",
       "257    0\n",
       "170    0\n",
       "286    0\n",
       "90     0\n",
       "294    0\n",
       "199    0\n",
       "224    0\n",
       "89     0\n",
       "51     1\n",
       "209    0\n",
       "16     1\n",
       "138    0\n",
       "110    1\n",
       "163    1\n",
       "78     0\n",
       "250    0\n",
       "247    0\n",
       "18     1\n",
       "167    1\n",
       "123    0\n",
       "225    0\n",
       "24     1\n",
       "142    0\n",
       "145    0\n",
       "148    1\n",
       "291    0\n",
       "190    0\n",
       "66     1\n",
       "201    0\n",
       "210    0\n",
       "187    1\n",
       "29     1\n",
       "144    1\n",
       "10     1\n",
       "152    0\n",
       "131    0\n",
       "135    0\n",
       "Name: DEATH_EVENT, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007c58a1-8fe9-4ca9-bc7c-095781a45323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7222222222222222\n",
      "Precision: 0.5769230769230769\n",
      "Recall: 0.5172413793103449\n",
      "F1-Score: 0.5454545454545454\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", accuracy_score(y_test, preds))\n",
    "print(\"Precision:\", precision_score(y_test, preds))\n",
    "print(\"Recall:\", recall_score(y_test, preds))\n",
    "print(\"F1-Score:\", f1_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcae629-83dc-4a32-93da-24811369b464",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b416d56-6572-43e8-9ea5-de9b06aa80bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", accuracy_score(y_train, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c39d56f-5ee4-44e5-b552-15922ee1b0d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gini'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmin_samples_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmin_samples_leaf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmin_weight_fraction_leaf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sqrt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_leaf_nodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmin_impurity_decrease\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbootstrap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0moob_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mwarm_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mccp_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmonotonic_cst\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "A random forest classifier.\n",
       "\n",
       "A random forest is a meta estimator that fits a number of decision tree\n",
       "classifiers on various sub-samples of the dataset and uses averaging to\n",
       "improve the predictive accuracy and control over-fitting.\n",
       "Trees in the forest use the best split strategy, i.e. equivalent to passing\n",
       "`splitter=\"best\"` to the underlying :class:`~sklearn.tree.DecisionTreeClassifier`.\n",
       "The sub-sample size is controlled with the `max_samples` parameter if\n",
       "`bootstrap=True` (default), otherwise the whole dataset is used to build\n",
       "each tree.\n",
       "\n",
       "For a comparison between tree-based ensemble models see the example\n",
       ":ref:`sphx_glr_auto_examples_ensemble_plot_forest_hist_grad_boosting_comparison.py`.\n",
       "\n",
       "Read more in the :ref:`User Guide <forest>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "n_estimators : int, default=100\n",
       "    The number of trees in the forest.\n",
       "\n",
       "    .. versionchanged:: 0.22\n",
       "       The default value of ``n_estimators`` changed from 10 to 100\n",
       "       in 0.22.\n",
       "\n",
       "criterion : {\"gini\", \"entropy\", \"log_loss\"}, default=\"gini\"\n",
       "    The function to measure the quality of a split. Supported criteria are\n",
       "    \"gini\" for the Gini impurity and \"log_loss\" and \"entropy\" both for the\n",
       "    Shannon information gain, see :ref:`tree_mathematical_formulation`.\n",
       "    Note: This parameter is tree-specific.\n",
       "\n",
       "max_depth : int, default=None\n",
       "    The maximum depth of the tree. If None, then nodes are expanded until\n",
       "    all leaves are pure or until all leaves contain less than\n",
       "    min_samples_split samples.\n",
       "\n",
       "min_samples_split : int or float, default=2\n",
       "    The minimum number of samples required to split an internal node:\n",
       "\n",
       "    - If int, then consider `min_samples_split` as the minimum number.\n",
       "    - If float, then `min_samples_split` is a fraction and\n",
       "      `ceil(min_samples_split * n_samples)` are the minimum\n",
       "      number of samples for each split.\n",
       "\n",
       "    .. versionchanged:: 0.18\n",
       "       Added float values for fractions.\n",
       "\n",
       "min_samples_leaf : int or float, default=1\n",
       "    The minimum number of samples required to be at a leaf node.\n",
       "    A split point at any depth will only be considered if it leaves at\n",
       "    least ``min_samples_leaf`` training samples in each of the left and\n",
       "    right branches.  This may have the effect of smoothing the model,\n",
       "    especially in regression.\n",
       "\n",
       "    - If int, then consider `min_samples_leaf` as the minimum number.\n",
       "    - If float, then `min_samples_leaf` is a fraction and\n",
       "      `ceil(min_samples_leaf * n_samples)` are the minimum\n",
       "      number of samples for each node.\n",
       "\n",
       "    .. versionchanged:: 0.18\n",
       "       Added float values for fractions.\n",
       "\n",
       "min_weight_fraction_leaf : float, default=0.0\n",
       "    The minimum weighted fraction of the sum total of weights (of all\n",
       "    the input samples) required to be at a leaf node. Samples have\n",
       "    equal weight when sample_weight is not provided.\n",
       "\n",
       "max_features : {\"sqrt\", \"log2\", None}, int or float, default=\"sqrt\"\n",
       "    The number of features to consider when looking for the best split:\n",
       "\n",
       "    - If int, then consider `max_features` features at each split.\n",
       "    - If float, then `max_features` is a fraction and\n",
       "      `max(1, int(max_features * n_features_in_))` features are considered at each\n",
       "      split.\n",
       "    - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
       "    - If \"log2\", then `max_features=log2(n_features)`.\n",
       "    - If None, then `max_features=n_features`.\n",
       "\n",
       "    .. versionchanged:: 1.1\n",
       "        The default of `max_features` changed from `\"auto\"` to `\"sqrt\"`.\n",
       "\n",
       "    Note: the search for a split does not stop until at least one\n",
       "    valid partition of the node samples is found, even if it requires to\n",
       "    effectively inspect more than ``max_features`` features.\n",
       "\n",
       "max_leaf_nodes : int, default=None\n",
       "    Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
       "    Best nodes are defined as relative reduction in impurity.\n",
       "    If None then unlimited number of leaf nodes.\n",
       "\n",
       "min_impurity_decrease : float, default=0.0\n",
       "    A node will be split if this split induces a decrease of the impurity\n",
       "    greater than or equal to this value.\n",
       "\n",
       "    The weighted impurity decrease equation is the following::\n",
       "\n",
       "        N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
       "                            - N_t_L / N_t * left_impurity)\n",
       "\n",
       "    where ``N`` is the total number of samples, ``N_t`` is the number of\n",
       "    samples at the current node, ``N_t_L`` is the number of samples in the\n",
       "    left child, and ``N_t_R`` is the number of samples in the right child.\n",
       "\n",
       "    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
       "    if ``sample_weight`` is passed.\n",
       "\n",
       "    .. versionadded:: 0.19\n",
       "\n",
       "bootstrap : bool, default=True\n",
       "    Whether bootstrap samples are used when building trees. If False, the\n",
       "    whole dataset is used to build each tree.\n",
       "\n",
       "oob_score : bool or callable, default=False\n",
       "    Whether to use out-of-bag samples to estimate the generalization score.\n",
       "    By default, :func:`~sklearn.metrics.accuracy_score` is used.\n",
       "    Provide a callable with signature `metric(y_true, y_pred)` to use a\n",
       "    custom metric. Only available if `bootstrap=True`.\n",
       "\n",
       "n_jobs : int, default=None\n",
       "    The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n",
       "    :meth:`decision_path` and :meth:`apply` are all parallelized over the\n",
       "    trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
       "    context. ``-1`` means using all processors. See :term:`Glossary\n",
       "    <n_jobs>` for more details.\n",
       "\n",
       "random_state : int, RandomState instance or None, default=None\n",
       "    Controls both the randomness of the bootstrapping of the samples used\n",
       "    when building trees (if ``bootstrap=True``) and the sampling of the\n",
       "    features to consider when looking for the best split at each node\n",
       "    (if ``max_features < n_features``).\n",
       "    See :term:`Glossary <random_state>` for details.\n",
       "\n",
       "verbose : int, default=0\n",
       "    Controls the verbosity when fitting and predicting.\n",
       "\n",
       "warm_start : bool, default=False\n",
       "    When set to ``True``, reuse the solution of the previous call to fit\n",
       "    and add more estimators to the ensemble, otherwise, just fit a whole\n",
       "    new forest. See :term:`Glossary <warm_start>` and\n",
       "    :ref:`tree_ensemble_warm_start` for details.\n",
       "\n",
       "class_weight : {\"balanced\", \"balanced_subsample\"}, dict or list of dicts,             default=None\n",
       "    Weights associated with classes in the form ``{class_label: weight}``.\n",
       "    If not given, all classes are supposed to have weight one. For\n",
       "    multi-output problems, a list of dicts can be provided in the same\n",
       "    order as the columns of y.\n",
       "\n",
       "    Note that for multioutput (including multilabel) weights should be\n",
       "    defined for each class of every column in its own dict. For example,\n",
       "    for four-class multilabel classification weights should be\n",
       "    [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n",
       "    [{1:1}, {2:5}, {3:1}, {4:1}].\n",
       "\n",
       "    The \"balanced\" mode uses the values of y to automatically adjust\n",
       "    weights inversely proportional to class frequencies in the input data\n",
       "    as ``n_samples / (n_classes * np.bincount(y))``\n",
       "\n",
       "    The \"balanced_subsample\" mode is the same as \"balanced\" except that\n",
       "    weights are computed based on the bootstrap sample for every tree\n",
       "    grown.\n",
       "\n",
       "    For multi-output, the weights of each column of y will be multiplied.\n",
       "\n",
       "    Note that these weights will be multiplied with sample_weight (passed\n",
       "    through the fit method) if sample_weight is specified.\n",
       "\n",
       "ccp_alpha : non-negative float, default=0.0\n",
       "    Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
       "    subtree with the largest cost complexity that is smaller than\n",
       "    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
       "    :ref:`minimal_cost_complexity_pruning` for details. See\n",
       "    :ref:`sphx_glr_auto_examples_tree_plot_cost_complexity_pruning.py`\n",
       "    for an example of such pruning.\n",
       "\n",
       "    .. versionadded:: 0.22\n",
       "\n",
       "max_samples : int or float, default=None\n",
       "    If bootstrap is True, the number of samples to draw from X\n",
       "    to train each base estimator.\n",
       "\n",
       "    - If None (default), then draw `X.shape[0]` samples.\n",
       "    - If int, then draw `max_samples` samples.\n",
       "    - If float, then draw `max(round(n_samples * max_samples), 1)` samples. Thus,\n",
       "      `max_samples` should be in the interval `(0.0, 1.0]`.\n",
       "\n",
       "    .. versionadded:: 0.22\n",
       "\n",
       "monotonic_cst : array-like of int of shape (n_features), default=None\n",
       "    Indicates the monotonicity constraint to enforce on each feature.\n",
       "      - 1: monotonic increase\n",
       "      - 0: no constraint\n",
       "      - -1: monotonic decrease\n",
       "\n",
       "    If monotonic_cst is None, no constraints are applied.\n",
       "\n",
       "    Monotonicity constraints are not supported for:\n",
       "      - multiclass classifications (i.e. when `n_classes > 2`),\n",
       "      - multioutput classifications (i.e. when `n_outputs_ > 1`),\n",
       "      - classifications trained on data with missing values.\n",
       "\n",
       "    The constraints hold over the probability of the positive class.\n",
       "\n",
       "    Read more in the :ref:`User Guide <monotonic_cst_gbdt>`.\n",
       "\n",
       "    .. versionadded:: 1.4\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "estimator_ : :class:`~sklearn.tree.DecisionTreeClassifier`\n",
       "    The child estimator template used to create the collection of fitted\n",
       "    sub-estimators.\n",
       "\n",
       "    .. versionadded:: 1.2\n",
       "       `base_estimator_` was renamed to `estimator_`.\n",
       "\n",
       "estimators_ : list of DecisionTreeClassifier\n",
       "    The collection of fitted sub-estimators.\n",
       "\n",
       "classes_ : ndarray of shape (n_classes,) or a list of such arrays\n",
       "    The classes labels (single output problem), or a list of arrays of\n",
       "    class labels (multi-output problem).\n",
       "\n",
       "n_classes_ : int or list\n",
       "    The number of classes (single output problem), or a list containing the\n",
       "    number of classes for each output (multi-output problem).\n",
       "\n",
       "n_features_in_ : int\n",
       "    Number of features seen during :term:`fit`.\n",
       "\n",
       "    .. versionadded:: 0.24\n",
       "\n",
       "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
       "    Names of features seen during :term:`fit`. Defined only when `X`\n",
       "    has feature names that are all strings.\n",
       "\n",
       "    .. versionadded:: 1.0\n",
       "\n",
       "n_outputs_ : int\n",
       "    The number of outputs when ``fit`` is performed.\n",
       "\n",
       "feature_importances_ : ndarray of shape (n_features,)\n",
       "    The impurity-based feature importances.\n",
       "    The higher, the more important the feature.\n",
       "    The importance of a feature is computed as the (normalized)\n",
       "    total reduction of the criterion brought by that feature.  It is also\n",
       "    known as the Gini importance.\n",
       "\n",
       "    Warning: impurity-based feature importances can be misleading for\n",
       "    high cardinality features (many unique values). See\n",
       "    :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
       "\n",
       "oob_score_ : float\n",
       "    Score of the training dataset obtained using an out-of-bag estimate.\n",
       "    This attribute exists only when ``oob_score`` is True.\n",
       "\n",
       "oob_decision_function_ : ndarray of shape (n_samples, n_classes) or             (n_samples, n_classes, n_outputs)\n",
       "    Decision function computed with out-of-bag estimate on the training\n",
       "    set. If n_estimators is small it might be possible that a data point\n",
       "    was never left out during the bootstrap. In this case,\n",
       "    `oob_decision_function_` might contain NaN. This attribute exists\n",
       "    only when ``oob_score`` is True.\n",
       "\n",
       "estimators_samples_ : list of arrays\n",
       "    The subset of drawn samples (i.e., the in-bag samples) for each base\n",
       "    estimator. Each subset is defined by an array of the indices selected.\n",
       "\n",
       "    .. versionadded:: 1.4\n",
       "\n",
       "See Also\n",
       "--------\n",
       "sklearn.tree.DecisionTreeClassifier : A decision tree classifier.\n",
       "sklearn.ensemble.ExtraTreesClassifier : Ensemble of extremely randomized\n",
       "    tree classifiers.\n",
       "sklearn.ensemble.HistGradientBoostingClassifier : A Histogram-based Gradient\n",
       "    Boosting Classification Tree, very fast for big datasets (n_samples >=\n",
       "    10_000).\n",
       "\n",
       "Notes\n",
       "-----\n",
       "The default values for the parameters controlling the size of the trees\n",
       "(e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
       "unpruned trees which can potentially be very large on some data sets. To\n",
       "reduce memory consumption, the complexity and size of the trees should be\n",
       "controlled by setting those parameter values.\n",
       "\n",
       "The features are always randomly permuted at each split. Therefore,\n",
       "the best found split may vary, even with the same training data,\n",
       "``max_features=n_features`` and ``bootstrap=False``, if the improvement\n",
       "of the criterion is identical for several splits enumerated during the\n",
       "search of the best split. To obtain a deterministic behaviour during\n",
       "fitting, ``random_state`` has to be fixed.\n",
       "\n",
       "References\n",
       "----------\n",
       ".. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from sklearn.ensemble import RandomForestClassifier\n",
       ">>> from sklearn.datasets import make_classification\n",
       ">>> X, y = make_classification(n_samples=1000, n_features=4,\n",
       "...                            n_informative=2, n_redundant=0,\n",
       "...                            random_state=0, shuffle=False)\n",
       ">>> clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
       ">>> clf.fit(X, y)\n",
       "RandomForestClassifier(...)\n",
       ">>> print(clf.predict([[0, 0, 0, 0]]))\n",
       "[1]\n",
       "\u001b[0;31mFile:\u001b[0m           /opt/anaconda3/lib/python3.13/site-packages/sklearn/ensemble/_forest.py\n",
       "\u001b[0;31mType:\u001b[0m           ABCMeta\n",
       "\u001b[0;31mSubclasses:\u001b[0m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RandomForestClassifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d049677d-d44a-43f3-bebe-64baca1f3ad3",
   "metadata": {},
   "source": [
    "# Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcd629b-5f0b-4263-ade0-1841d43e8b2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[31mInit signature:\u001b[39m\n",
       "SVC(\n",
       "    *,\n",
       "    C=\u001b[32m1.0\u001b[39m,\n",
       "    kernel=\u001b[33m'rbf'\u001b[39m,\n",
       "    degree=\u001b[32m3\u001b[39m,\n",
       "    gamma=\u001b[33m'scale'\u001b[39m,\n",
       "    coef0=\u001b[32m0.0\u001b[39m,\n",
       "    shrinking=\u001b[38;5;28;01mTrue\u001b[39;00m,\n",
       "    probability=\u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       "    tol=\u001b[32m0.001\u001b[39m,\n",
       "    cache_size=\u001b[32m200\u001b[39m,\n",
       "    class_weight=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    verbose=\u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       "    max_iter=-\u001b[32m1\u001b[39m,\n",
       "    decision_function_shape=\u001b[33m'ovr'\u001b[39m,\n",
       "    break_ties=\u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       "    random_state=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       ")\n",
       "\u001b[31mDocstring:\u001b[39m     \n",
       "C-Support Vector Classification.\n",
       "\n",
       "The implementation is based on libsvm. The fit time scales at least\n",
       "quadratically with the number of samples and may be impractical\n",
       "beyond tens of thousands of samples. For large datasets\n",
       "consider using :class:`~sklearn.svm.LinearSVC` or\n",
       ":class:`~sklearn.linear_model.SGDClassifier` instead, possibly after a\n",
       ":class:`~sklearn.kernel_approximation.Nystroem` transformer or\n",
       "other :ref:`kernel_approximation`.\n",
       "\n",
       "The multiclass support is handled according to a one-vs-one scheme.\n",
       "\n",
       "For details on the precise mathematical formulation of the provided\n",
       "kernel functions and how `gamma`, `coef0` and `degree` affect each\n",
       "other, see the corresponding section in the narrative documentation:\n",
       ":ref:`svm_kernels`.\n",
       "\n",
       "To learn how to tune SVC's hyperparameters, see the following example:\n",
       ":ref:`sphx_glr_auto_examples_model_selection_plot_nested_cross_validation_iris.py`\n",
       "\n",
       "Read more in the :ref:`User Guide <svm_classification>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "C : float, default=1.0\n",
       "    Regularization parameter. The strength of the regularization is\n",
       "    inversely proportional to C. Must be strictly positive. The penalty\n",
       "    is a squared l2 penalty. For an intuitive visualization of the effects\n",
       "    of scaling the regularization parameter C, see\n",
       "    :ref:`sphx_glr_auto_examples_svm_plot_svm_scale_c.py`.\n",
       "\n",
       "kernel : {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'} or callable,          default='rbf'\n",
       "    Specifies the kernel type to be used in the algorithm. If\n",
       "    none is given, 'rbf' will be used. If a callable is given it is used to\n",
       "    pre-compute the kernel matrix from data matrices; that matrix should be\n",
       "    an array of shape ``(n_samples, n_samples)``. For an intuitive\n",
       "    visualization of different kernel types see\n",
       "    :ref:`sphx_glr_auto_examples_svm_plot_svm_kernels.py`.\n",
       "\n",
       "degree : int, default=3\n",
       "    Degree of the polynomial kernel function ('poly').\n",
       "    Must be non-negative. Ignored by all other kernels.\n",
       "\n",
       "gamma : {'scale', 'auto'} or float, default='scale'\n",
       "    Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\n",
       "\n",
       "    - if ``gamma='scale'`` (default) is passed then it uses\n",
       "      1 / (n_features * X.var()) as value of gamma,\n",
       "    - if 'auto', uses 1 / n_features\n",
       "    - if float, must be non-negative.\n",
       "\n",
       "    .. versionchanged:: 0.22\n",
       "       The default value of ``gamma`` changed from 'auto' to 'scale'.\n",
       "\n",
       "coef0 : float, default=0.0\n",
       "    Independent term in kernel function.\n",
       "    It is only significant in 'poly' and 'sigmoid'.\n",
       "\n",
       "shrinking : bool, default=True\n",
       "    Whether to use the shrinking heuristic.\n",
       "    See the :ref:`User Guide <shrinking_svm>`.\n",
       "\n",
       "probability : bool, default=False\n",
       "    Whether to enable probability estimates. This must be enabled prior\n",
       "    to calling `fit`, will slow down that method as it internally uses\n",
       "    5-fold cross-validation, and `predict_proba` may be inconsistent with\n",
       "    `predict`. Read more in the :ref:`User Guide <scores_probabilities>`.\n",
       "\n",
       "tol : float, default=1e-3\n",
       "    Tolerance for stopping criterion.\n",
       "\n",
       "cache_size : float, default=200\n",
       "    Specify the size of the kernel cache (in MB).\n",
       "\n",
       "class_weight : dict or 'balanced', default=None\n",
       "    Set the parameter C of class i to class_weight[i]*C for\n",
       "    SVC. If not given, all classes are supposed to have\n",
       "    weight one.\n",
       "    The \"balanced\" mode uses the values of y to automatically adjust\n",
       "    weights inversely proportional to class frequencies in the input data\n",
       "    as ``n_samples / (n_classes * np.bincount(y))``.\n",
       "\n",
       "verbose : bool, default=False\n",
       "    Enable verbose output. Note that this setting takes advantage of a\n",
       "    per-process runtime setting in libsvm that, if enabled, may not work\n",
       "    properly in a multithreaded context.\n",
       "\n",
       "max_iter : int, default=-1\n",
       "    Hard limit on iterations within solver, or -1 for no limit.\n",
       "\n",
       "decision_function_shape : {'ovo', 'ovr'}, default='ovr'\n",
       "    Whether to return a one-vs-rest ('ovr') decision function of shape\n",
       "    (n_samples, n_classes) as all other classifiers, or the original\n",
       "    one-vs-one ('ovo') decision function of libsvm which has shape\n",
       "    (n_samples, n_classes * (n_classes - 1) / 2). However, note that\n",
       "    internally, one-vs-one ('ovo') is always used as a multi-class strategy\n",
       "    to train models; an ovr matrix is only constructed from the ovo matrix.\n",
       "    The parameter is ignored for binary classification.\n",
       "\n",
       "    .. versionchanged:: 0.19\n",
       "        decision_function_shape is 'ovr' by default.\n",
       "\n",
       "    .. versionadded:: 0.17\n",
       "       *decision_function_shape='ovr'* is recommended.\n",
       "\n",
       "    .. versionchanged:: 0.17\n",
       "       Deprecated *decision_function_shape='ovo' and None*.\n",
       "\n",
       "break_ties : bool, default=False\n",
       "    If true, ``decision_function_shape='ovr'``, and number of classes > 2,\n",
       "    :term:`predict` will break ties according to the confidence values of\n",
       "    :term:`decision_function`; otherwise the first class among the tied\n",
       "    classes is returned. Please note that breaking ties comes at a\n",
       "    relatively high computational cost compared to a simple predict. See\n",
       "    :ref:`sphx_glr_auto_examples_svm_plot_svm_tie_breaking.py` for an\n",
       "    example of its usage with ``decision_function_shape='ovr'``.\n",
       "\n",
       "    .. versionadded:: 0.22\n",
       "\n",
       "random_state : int, RandomState instance or None, default=None\n",
       "    Controls the pseudo random number generation for shuffling the data for\n",
       "    probability estimates. Ignored when `probability` is False.\n",
       "    Pass an int for reproducible output across multiple function calls.\n",
       "    See :term:`Glossary <random_state>`.\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "class_weight_ : ndarray of shape (n_classes,)\n",
       "    Multipliers of parameter C for each class.\n",
       "    Computed based on the ``class_weight`` parameter.\n",
       "\n",
       "classes_ : ndarray of shape (n_classes,)\n",
       "    The classes labels.\n",
       "\n",
       "coef_ : ndarray of shape (n_classes * (n_classes - 1) / 2, n_features)\n",
       "    Weights assigned to the features (coefficients in the primal\n",
       "    problem). This is only available in the case of a linear kernel.\n",
       "\n",
       "    `coef_` is a readonly property derived from `dual_coef_` and\n",
       "    `support_vectors_`.\n",
       "\n",
       "dual_coef_ : ndarray of shape (n_classes -1, n_SV)\n",
       "    Dual coefficients of the support vector in the decision\n",
       "    function (see :ref:`sgd_mathematical_formulation`), multiplied by\n",
       "    their targets.\n",
       "    For multiclass, coefficient for all 1-vs-1 classifiers.\n",
       "    The layout of the coefficients in the multiclass case is somewhat\n",
       "    non-trivial. See the :ref:`multi-class section of the User Guide\n",
       "    <svm_multi_class>` for details.\n",
       "\n",
       "fit_status_ : int\n",
       "    0 if correctly fitted, 1 otherwise (will raise warning)\n",
       "\n",
       "intercept_ : ndarray of shape (n_classes * (n_classes - 1) / 2,)\n",
       "    Constants in decision function.\n",
       "\n",
       "n_features_in_ : int\n",
       "    Number of features seen during :term:`fit`.\n",
       "\n",
       "    .. versionadded:: 0.24\n",
       "\n",
       "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
       "    Names of features seen during :term:`fit`. Defined only when `X`\n",
       "    has feature names that are all strings.\n",
       "\n",
       "    .. versionadded:: 1.0\n",
       "\n",
       "n_iter_ : ndarray of shape (n_classes * (n_classes - 1) // 2,)\n",
       "    Number of iterations run by the optimization routine to fit the model.\n",
       "    The shape of this attribute depends on the number of models optimized\n",
       "    which in turn depends on the number of classes.\n",
       "\n",
       "    .. versionadded:: 1.1\n",
       "\n",
       "support_ : ndarray of shape (n_SV)\n",
       "    Indices of support vectors.\n",
       "\n",
       "support_vectors_ : ndarray of shape (n_SV, n_features)\n",
       "    Support vectors. An empty array if kernel is precomputed.\n",
       "\n",
       "n_support_ : ndarray of shape (n_classes,), dtype=int32\n",
       "    Number of support vectors for each class.\n",
       "\n",
       "probA_ : ndarray of shape (n_classes * (n_classes - 1) / 2)\n",
       "probB_ : ndarray of shape (n_classes * (n_classes - 1) / 2)\n",
       "    If `probability=True`, it corresponds to the parameters learned in\n",
       "    Platt scaling to produce probability estimates from decision values.\n",
       "    If `probability=False`, it's an empty array. Platt scaling uses the\n",
       "    logistic function\n",
       "    ``1 / (1 + exp(decision_value * probA_ + probB_))``\n",
       "    where ``probA_`` and ``probB_`` are learned from the dataset [2]_. For\n",
       "    more information on the multiclass case and training procedure see\n",
       "    section 8 of [1]_.\n",
       "\n",
       "shape_fit_ : tuple of int of shape (n_dimensions_of_X,)\n",
       "    Array dimensions of training vector ``X``.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "SVR : Support Vector Machine for Regression implemented using libsvm.\n",
       "\n",
       "LinearSVC : Scalable Linear Support Vector Machine for classification\n",
       "    implemented using liblinear. Check the See Also section of\n",
       "    LinearSVC for more comparison element.\n",
       "\n",
       "References\n",
       "----------\n",
       ".. [1] `LIBSVM: A Library for Support Vector Machines\n",
       "    <http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf>`_\n",
       "\n",
       ".. [2] `Platt, John (1999). \"Probabilistic Outputs for Support Vector\n",
       "    Machines and Comparisons to Regularized Likelihood Methods\"\n",
       "    <https://citeseerx.ist.psu.edu/doc_view/pid/42e5ed832d4310ce4378c44d05570439df28a393>`_\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> import numpy as np\n",
       ">>> from sklearn.pipeline import make_pipeline\n",
       ">>> from sklearn.preprocessing import StandardScaler\n",
       ">>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n",
       ">>> y = np.array([1, 1, 2, 2])\n",
       ">>> from sklearn.svm import SVC\n",
       ">>> clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
       ">>> clf.fit(X, y)\n",
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('svc', SVC(gamma='auto'))])\n",
       "\n",
       ">>> print(clf.predict([[-0.8, -1]]))\n",
       "[1]\n",
       "\n",
       "For a comparison of the SVC with other classifiers see:\n",
       ":ref:`sphx_glr_auto_examples_classification_plot_classification_probability.py`.\n",
       "\u001b[31mFile:\u001b[39m           /opt/anaconda3/envs/vmb_env_py311/lib/python3.11/site-packages/sklearn/svm/_classes.py\n",
       "\u001b[31mType:\u001b[39m           ABCMeta\n",
       "\u001b[31mSubclasses:\u001b[39m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SVC?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a52a666-c0aa-49de-a4fc-1eccf58ac1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7\n",
      "Precision: 0.5454545454545454\n",
      "Recall: 0.41379310344827586\n",
      "F1-Score: 0.47058823529411764\n"
     ]
    }
   ],
   "source": [
    "#Linear SVM\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "model = SVC(kernel='linear')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "preds = model.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, preds))\n",
    "print(\"Precision:\", precision_score(y_test, preds))\n",
    "print(\"Recall:\", recall_score(y_test, preds))\n",
    "print(\"F1-Score:\", f1_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1299dab4-d983-44e8-9e8c-17410d79c494",
   "metadata": {},
   "source": [
    "# Excercise: Run SVM with RBF kernel and report metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75c294a-d53b-4809-a441-baef9d025a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7333333333333333\n",
      "Precision: 0.6190476190476191\n",
      "Recall: 0.4482758620689655\n",
      "F1-Score: 0.52\n"
     ]
    }
   ],
   "source": [
    "#Write your code here\n",
    "from sklearn.svm import SVC\n",
    "model = SVC(kernel='rbf')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "preds = model.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, preds))\n",
    "print(\"Precision:\", precision_score(y_test, preds))\n",
    "print(\"Recall:\", recall_score(y_test, preds))\n",
    "print(\"F1-Score:\", f1_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729cea3f-ebc7-4758-8a7c-13eac5cc5e63",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fab3c5-8485-4b8d-97c1-9e41e84c0544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6666666666666666\n",
      "Precision: 0.47058823529411764\n",
      "Recall: 0.27586206896551724\n",
      "F1-Score: 0.34782608695652173\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "model = KNeighborsClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "preds = model.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, preds))\n",
    "print(\"Precision:\", precision_score(y_test, preds))\n",
    "print(\"Recall:\", recall_score(y_test, preds))\n",
    "print(\"F1-Score:\", f1_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a187ca3-6e75-4ae7-8a09-625f0235cc2a",
   "metadata": {},
   "source": [
    "# Dummy classifier\n",
    "\n",
    "# Baseline Model: Dummy Classifier\n",
    "\n",
    "## What is a Baseline Model?\n",
    "\n",
    "A **baseline model** is a simple, often trivial model used as a reference point to evaluate the performance of more complex machine learning models.\n",
    "\n",
    "**Why is it important?**\n",
    "- Provides a **minimum performance threshold** - any real model should beat the baseline\n",
    "- Helps detect **overfitting** - if your fancy model barely beats the baseline, something is wrong\n",
    "- Establishes **expectations** - shows what luck alone can achieve\n",
    "- Useful for **imbalanced datasets** - reveals class distribution bias\n",
    "- **Sanity check** - if you can't beat the baseline, reconsider your approach\n",
    "\n",
    "**Our Baseline Question:** If we just always guess \"survived\" (the majority class), what accuracy would we get?\n",
    "- **Answer:** 67.8% (proportion of class 0 in test set)\n",
    "\n",
    "This means any real ML model must get > 67.8% accuracy to be useful!\n",
    "\n",
    "\n",
    "## Dummy Classifier Strategies\n",
    "\n",
    "DummyClassifier provides different simple strategies:\n",
    "\n",
    "| Strategy | What it does | Use case |\n",
    "|----------|-------------|----------|\n",
    "| **most_frequent** | Always predict the most common class | Baseline for imbalanced data |\n",
    "| **stratified** | Predicts randomly but matches class distribution | Compare to random guessing |\n",
    "| **uniform** | Completely random predictions | See what true random guessing gives |\n",
    "| **constant** | Always predicts the same class you specify | Custom baseline |\n",
    "\n",
    "**For our heart failure dataset:**\n",
    "- **most_frequent strategy:** Always predicts \"survived\" (class 0) → ~67.8% accuracy\n",
    "- **stratified strategy:** Randomly guesses with 67.8% \"survived\" and 32.2% \"death\" → accuracy varies\n",
    "- **uniform strategy:** 50/50 random guess → ~50% accuracy (worse than most_frequent!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90430180-e864-44ee-8965-d893eb80e92f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mDummyClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'prior'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "DummyClassifier makes predictions that ignore the input features.\n",
       "\n",
       "This classifier serves as a simple baseline to compare against other more\n",
       "complex classifiers.\n",
       "\n",
       "The specific behavior of the baseline is selected with the `strategy`\n",
       "parameter.\n",
       "\n",
       "All strategies make predictions that ignore the input feature values passed\n",
       "as the `X` argument to `fit` and `predict`. The predictions, however,\n",
       "typically depend on values observed in the `y` parameter passed to `fit`.\n",
       "\n",
       "Note that the \"stratified\" and \"uniform\" strategies lead to\n",
       "non-deterministic predictions that can be rendered deterministic by setting\n",
       "the `random_state` parameter if needed. The other strategies are naturally\n",
       "deterministic and, once fit, always return the same constant prediction\n",
       "for any value of `X`.\n",
       "\n",
       "Read more in the :ref:`User Guide <dummy_estimators>`.\n",
       "\n",
       ".. versionadded:: 0.13\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "strategy : {\"most_frequent\", \"prior\", \"stratified\", \"uniform\",             \"constant\"}, default=\"prior\"\n",
       "    Strategy to use to generate predictions.\n",
       "\n",
       "    * \"most_frequent\": the `predict` method always returns the most\n",
       "      frequent class label in the observed `y` argument passed to `fit`.\n",
       "      The `predict_proba` method returns the matching one-hot encoded\n",
       "      vector.\n",
       "    * \"prior\": the `predict` method always returns the most frequent\n",
       "      class label in the observed `y` argument passed to `fit` (like\n",
       "      \"most_frequent\"). ``predict_proba`` always returns the empirical\n",
       "      class distribution of `y` also known as the empirical class prior\n",
       "      distribution.\n",
       "    * \"stratified\": the `predict_proba` method randomly samples one-hot\n",
       "      vectors from a multinomial distribution parametrized by the empirical\n",
       "      class prior probabilities.\n",
       "      The `predict` method returns the class label which got probability\n",
       "      one in the one-hot vector of `predict_proba`.\n",
       "      Each sampled row of both methods is therefore independent and\n",
       "      identically distributed.\n",
       "    * \"uniform\": generates predictions uniformly at random from the list\n",
       "      of unique classes observed in `y`, i.e. each class has equal\n",
       "      probability.\n",
       "    * \"constant\": always predicts a constant label that is provided by\n",
       "      the user. This is useful for metrics that evaluate a non-majority\n",
       "      class.\n",
       "\n",
       "      .. versionchanged:: 0.24\n",
       "         The default value of `strategy` has changed to \"prior\" in version\n",
       "         0.24.\n",
       "\n",
       "random_state : int, RandomState instance or None, default=None\n",
       "    Controls the randomness to generate the predictions when\n",
       "    ``strategy='stratified'`` or ``strategy='uniform'``.\n",
       "    Pass an int for reproducible output across multiple function calls.\n",
       "    See :term:`Glossary <random_state>`.\n",
       "\n",
       "constant : int or str or array-like of shape (n_outputs,), default=None\n",
       "    The explicit constant as predicted by the \"constant\" strategy. This\n",
       "    parameter is useful only for the \"constant\" strategy.\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "classes_ : ndarray of shape (n_classes,) or list of such arrays\n",
       "    Unique class labels observed in `y`. For multi-output classification\n",
       "    problems, this attribute is a list of arrays as each output has an\n",
       "    independent set of possible classes.\n",
       "\n",
       "n_classes_ : int or list of int\n",
       "    Number of label for each output.\n",
       "\n",
       "class_prior_ : ndarray of shape (n_classes,) or list of such arrays\n",
       "    Frequency of each class observed in `y`. For multioutput classification\n",
       "    problems, this is computed independently for each output.\n",
       "\n",
       "n_features_in_ : int\n",
       "    Number of features seen during :term:`fit`.\n",
       "\n",
       "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
       "    Names of features seen during :term:`fit`. Defined only when `X` has\n",
       "    feature names that are all strings.\n",
       "\n",
       "n_outputs_ : int\n",
       "    Number of outputs.\n",
       "\n",
       "sparse_output_ : bool\n",
       "    True if the array returned from predict is to be in sparse CSC format.\n",
       "    Is automatically set to True if the input `y` is passed in sparse\n",
       "    format.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "DummyRegressor : Regressor that makes predictions using simple rules.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> import numpy as np\n",
       ">>> from sklearn.dummy import DummyClassifier\n",
       ">>> X = np.array([-1, 1, 1, 1])\n",
       ">>> y = np.array([0, 1, 1, 1])\n",
       ">>> dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
       ">>> dummy_clf.fit(X, y)\n",
       "DummyClassifier(strategy='most_frequent')\n",
       ">>> dummy_clf.predict(X)\n",
       "array([1, 1, 1, 1])\n",
       ">>> dummy_clf.score(X, y)\n",
       "0.75\n",
       "\u001b[0;31mFile:\u001b[0m           /opt/anaconda3/lib/python3.13/site-packages/sklearn/dummy.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "DummyClassifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdcec69-8aad-449f-831e-b923a7e2c59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6777777777777778\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1-Score: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dummy_model = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy_model.fit(X_train, y_train)\n",
    "\n",
    "preds = dummy_model.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, preds))\n",
    "print(\"Precision:\", precision_score(y_test, preds))\n",
    "print(\"Recall:\", recall_score(y_test, preds))\n",
    "print(\"F1-Score:\", f1_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f35993-c173-4452-a32c-532303c58933",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Excercise: Design another classifier not mentioned above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d1c31f-a7e2-4f24-b56e-c6c8a4f53e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "preds = model.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, preds))\n",
    "print(\"Precision:\", precision_score(y_test, preds))\n",
    "print(\"Recall:\", recall_score(y_test, preds))\n",
    "print(\"F1-Score:\", f1_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ez13xtjsnnt",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# Train all models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'SVM (RBF)': SVC(kernel='rbf', probability=True),\n",
    "    'SVM (Linear)': SVC(kernel='linear', probability=True),\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'Dummy (Baseline)': DummyClassifier(strategy='most_frequent')\n",
    "}\n",
    "\n",
    "# Store results\n",
    "results = {}\n",
    "predictions = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    predictions[name] = preds\n",
    "    \n",
    "    results[name] = {\n",
    "        'Accuracy': accuracy_score(y_test, preds),\n",
    "        'Precision': precision_score(y_test, preds, zero_division=0),\n",
    "        'Recall': recall_score(y_test, preds, zero_division=0),\n",
    "        'F1-Score': f1_score(y_test, preds, zero_division=0)\n",
    "    }\n",
    "\n",
    "# Create results dataframe\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(results_df.round(4))\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "x4dqgdzwkah",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 1: Metrics Comparison Bar Chart\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "colors = plt.cm.Set3(np.linspace(0, 1, len(models)))\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    values = results_df[metric].values\n",
    "    x_pos = np.arange(len(models))\n",
    "    \n",
    "    bars = ax.bar(x_pos, values, color=colors, alpha=0.8, edgecolor='black')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.3f}',\n",
    "                ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "    \n",
    "    ax.axhline(y=0.678, color='red', linestyle='--', linewidth=2, label='Baseline (67.8%)')\n",
    "    ax.set_ylabel(metric, fontsize=11, fontweight='bold')\n",
    "    ax.set_title(f'{metric} Comparison', fontsize=12, fontweight='bold')\n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels(models.keys(), rotation=45, ha='right', fontsize=9)\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Insights:\")\n",
    "print(f\"Best Accuracy: {results_df['Accuracy'].idxmax()} ({results_df['Accuracy'].max():.4f})\")\n",
    "print(f\"Best Precision: {results_df['Precision'].idxmax()} ({results_df['Precision'].max():.4f})\")\n",
    "print(f\"Best Recall: {results_df['Recall'].idxmax()} ({results_df['Recall'].max():.4f})\")\n",
    "print(f\"Best F1-Score: {results_df['F1-Score'].idxmax()} ({results_df['F1-Score'].max():.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "k9ws0qd9a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 2: Confusion Matrices for Key Models\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "key_models = ['Dummy (Baseline)', 'Logistic Regression', 'Random Forest', \n",
    "              'SVM (RBF)', 'KNN', 'Naive Bayes']\n",
    "\n",
    "for idx, model_name in enumerate(key_models):\n",
    "    cm = confusion_matrix(y_test, predictions[model_name])\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],\n",
    "                cbar=False, annot_kws={'fontsize': 12, 'fontweight': 'bold'})\n",
    "    \n",
    "    axes[idx].set_title(f'{model_name}\\nAccuracy: {results[model_name][\"Accuracy\"]:.3f}',\n",
    "                        fontsize=11, fontweight='bold')\n",
    "    axes[idx].set_ylabel('True Label', fontsize=10)\n",
    "    axes[idx].set_xlabel('Predicted Label', fontsize=10)\n",
    "    axes[idx].set_xticklabels(['Survived', 'Died'])\n",
    "    axes[idx].set_yticklabels(['Survived', 'Died'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze confusion matrices\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CONFUSION MATRIX INTERPRETATION\")\n",
    "print(\"=\"*70)\n",
    "for model_name in key_models:\n",
    "    cm = confusion_matrix(y_test, predictions[model_name])\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  True Negatives (Survived correctly): {tn}\")\n",
    "    print(f\"  False Positives (Survived but predicted Died): {fp}\")\n",
    "    print(f\"  False Negatives (Died but predicted Survived): {fn}\")\n",
    "    print(f\"  True Positives (Died correctly): {tp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleee66uw6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 3: ROC Curves for Model Comparison\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "colors_roc = plt.cm.tab10(np.linspace(0, 1, len(models)))\n",
    "\n",
    "for (name, model), color in zip(models.items(), colors_roc):\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Get probability predictions\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    else:\n",
    "        y_proba = model.predict(X_test)\n",
    "    \n",
    "    # Calculate ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    ax.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.3f})',\n",
    "            linewidth=2.5, color=color)\n",
    "\n",
    "# Plot random classifier baseline\n",
    "ax.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random Classifier (AUC = 0.5)')\n",
    "\n",
    "ax.set_xlabel('False Positive Rate', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('True Positive Rate', fontsize=12, fontweight='bold')\n",
    "ax.set_title('ROC Curves: Model Comparison', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='lower right', fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlim([0, 1])\n",
    "ax.set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ROC-AUC SCORES\")\n",
    "print(\"=\"*70)\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    else:\n",
    "        y_proba = model.predict(X_test)\n",
    "    roc_auc = roc_auc_score(y_test, y_proba)\n",
    "    print(f\"{name:25} AUC-ROC: {roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nfubic0l1hg",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 4: Feature Importance from Random Forest\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "colors_importance = plt.cm.viridis(np.linspace(0.3, 0.9, len(feature_importance)))\n",
    "bars = ax.barh(feature_importance['Feature'], feature_importance['Importance'], \n",
    "               color=colors_importance, edgecolor='black', alpha=0.8)\n",
    "\n",
    "# Add value labels\n",
    "for idx, bar in enumerate(bars):\n",
    "    width = bar.get_width()\n",
    "    ax.text(width, bar.get_y() + bar.get_height()/2.,\n",
    "            f'{width:.4f}',\n",
    "            ha='left', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "ax.set_xlabel('Importance Score', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Random Forest: Feature Importance', fontsize=14, fontweight='bold')\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FEATURE IMPORTANCE RANKING\")\n",
    "print(\"=\"*70)\n",
    "for idx, row in feature_importance.iterrows():\n",
    "    print(f\"{row['Feature']:30} {row['Importance']:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "u2j1dc3bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 5: Class Distribution & Data Split\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Original class distribution\n",
    "original_dist = y.value_counts()\n",
    "axes[0].bar(['Survived (0)', 'Died (1)'], original_dist.values, \n",
    "            color=['steelblue', 'salmon'], alpha=0.7, edgecolor='black', linewidth=2)\n",
    "axes[0].set_ylabel('Count', fontsize=11, fontweight='bold')\n",
    "axes[0].set_title('Original Class Distribution', fontsize=12, fontweight='bold')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "for i, v in enumerate(original_dist.values):\n",
    "    axes[0].text(i, v + 3, str(v), ha='center', fontweight='bold', fontsize=11)\n",
    "\n",
    "# Train set class distribution\n",
    "train_dist = y_train.value_counts()\n",
    "axes[1].bar(['Survived (0)', 'Died (1)'], train_dist.values,\n",
    "            color=['steelblue', 'salmon'], alpha=0.7, edgecolor='black', linewidth=2)\n",
    "axes[1].set_ylabel('Count', fontsize=11, fontweight='bold')\n",
    "axes[1].set_title(f'Training Set ({len(y_train)} samples)', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "for i, v in enumerate(train_dist.values):\n",
    "    axes[1].text(i, v + 2, str(v), ha='center', fontweight='bold', fontsize=11)\n",
    "\n",
    "# Test set class distribution\n",
    "test_dist = y_test.value_counts()\n",
    "axes[2].bar(['Survived (0)', 'Died (1)'], test_dist.values,\n",
    "            color=['steelblue', 'salmon'], alpha=0.7, edgecolor='black', linewidth=2)\n",
    "axes[2].set_ylabel('Count', fontsize=11, fontweight='bold')\n",
    "axes[2].set_title(f'Test Set ({len(y_test)} samples)', fontsize=12, fontweight='bold')\n",
    "axes[2].grid(axis='y', alpha=0.3)\n",
    "for i, v in enumerate(test_dist.values):\n",
    "    axes[2].text(i, v + 1, str(v), ha='center', fontweight='bold', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATASET STATISTICS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total samples: {len(y)}\")\n",
    "print(f\"Training samples: {len(y_train)} ({len(y_train)/len(y)*100:.1f}%)\")\n",
    "print(f\"Test samples: {len(y_test)} ({len(y_test)/len(y)*100:.1f}%)\")\n",
    "print(f\"\\nClass distribution (Survived:Died)\")\n",
    "print(f\"Overall: {original_dist[0]}:{original_dist[1]} ({original_dist[0]/len(y)*100:.1f}%:{original_dist[1]/len(y)*100:.1f}%)\")\n",
    "print(f\"Training: {train_dist[0]}:{train_dist[1]} ({train_dist[0]/len(y_train)*100:.1f}%:{train_dist[1]/len(y_train)*100:.1f}%)\")\n",
    "print(f\"Test: {test_dist[0]}:{test_dist[1]} ({test_dist[0]/len(y_test)*100:.1f}%:{test_dist[1]/len(y_test)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wiogj1p5jl9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 6: Model Performance Heatmap\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "# Create heatmap\n",
    "sns.heatmap(results_df, annot=True, fmt='.4f', cmap='RdYlGn', center=0.5,\n",
    "            cbar_kws={'label': 'Score'}, linewidths=0.5, linecolor='gray',\n",
    "            ax=ax, vmin=0, vmax=1)\n",
    "\n",
    "ax.set_title('Model Performance Comparison Heatmap', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Model', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('Metrics', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL SELECTION GUIDE\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "CHOOSE YOUR MODEL BASED ON YOUR PRIORITY:\n",
    "\n",
    "1. **Best Overall (Balanced Performance)**\n",
    "   → Look for high F1-Score (balances precision & recall)\n",
    "\n",
    "2. **Minimize False Negatives (Critical)**\n",
    "   → Maximize Recall (catches all true positives)\n",
    "   → Example: Medical diagnosis (missing a disease is costly)\n",
    "\n",
    "3. **Minimize False Positives (Important)**\n",
    "   → Maximize Precision (minimize false alarms)\n",
    "   → Example: Spam detection (false positives annoy users)\n",
    "\n",
    "4. **Quick & Simple**\n",
    "   → Logistic Regression (interpretable, fast)\n",
    "\n",
    "5. **Maximum Accuracy**\n",
    "   → Random Forest or SVM (if accuracy is your metric)\n",
    "\n",
    "INTERPRETATION GUIDE:\n",
    "- Accuracy: % of correct predictions (can be misleading for imbalanced data)\n",
    "- Precision: Of predicted deaths, how many were correct?\n",
    "- Recall: Of actual deaths, how many did we catch?\n",
    "- F1-Score: Harmonic mean of precision & recall\n",
    "- AUC-ROC: Area under the ROC curve (best for imbalanced data)\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nBENCHMARK TO BEAT: 67.8% (Baseline - always predict 'Survived')\")\n",
    "print(\"Models exceeding baseline by >5%: Good performance\")\n",
    "print(\"Models exceeding baseline by >10%: Excellent performance\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}