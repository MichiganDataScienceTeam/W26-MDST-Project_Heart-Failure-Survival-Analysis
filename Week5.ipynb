{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9542668c-2086-4fa0-8588-07081236b162",
   "metadata": {},
   "source": [
    "# Optimize Machine Learning Models with Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3e85be6-07fd-4b05-b123-d4eda27587f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "df = pd.read_csv('./heart_failure_clinical_records_dataset.csv') #make sure to replace with your data directory\n",
    "df.drop(columns=['time'],inplace=True) #drop the time column which is not one of the informative features \n",
    "df_features = df.iloc[:,:-1]\n",
    "df_target = df['DEATH_EVENT']\n",
    "\n",
    "X = df_features\n",
    "y = df_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5474a902-b831-46e2-8cb2-0cc86bc5ec5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>anaemia</th>\n",
       "      <th>creatinine_phosphokinase</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>ejection_fraction</th>\n",
       "      <th>high_blood_pressure</th>\n",
       "      <th>platelets</th>\n",
       "      <th>serum_creatinine</th>\n",
       "      <th>serum_sodium</th>\n",
       "      <th>sex</th>\n",
       "      <th>smoking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75.0</td>\n",
       "      <td>0</td>\n",
       "      <td>582</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>265000.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>130</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>55.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7861</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>263358.03</td>\n",
       "      <td>1.1</td>\n",
       "      <td>136</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>65.0</td>\n",
       "      <td>0</td>\n",
       "      <td>146</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>162000.00</td>\n",
       "      <td>1.3</td>\n",
       "      <td>129</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1</td>\n",
       "      <td>111</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>210000.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>137</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>65.0</td>\n",
       "      <td>1</td>\n",
       "      <td>160</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>327000.00</td>\n",
       "      <td>2.7</td>\n",
       "      <td>116</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>155000.00</td>\n",
       "      <td>1.1</td>\n",
       "      <td>143</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>55.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1820</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>270000.00</td>\n",
       "      <td>1.2</td>\n",
       "      <td>139</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>45.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2060</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>742000.00</td>\n",
       "      <td>0.8</td>\n",
       "      <td>138</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>45.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2413</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>140000.00</td>\n",
       "      <td>1.4</td>\n",
       "      <td>140</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>196</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>395000.00</td>\n",
       "      <td>1.6</td>\n",
       "      <td>136</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>299 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      age  anaemia  creatinine_phosphokinase  diabetes  ejection_fraction  \\\n",
       "0    75.0        0                       582         0                 20   \n",
       "1    55.0        0                      7861         0                 38   \n",
       "2    65.0        0                       146         0                 20   \n",
       "3    50.0        1                       111         0                 20   \n",
       "4    65.0        1                       160         1                 20   \n",
       "..    ...      ...                       ...       ...                ...   \n",
       "294  62.0        0                        61         1                 38   \n",
       "295  55.0        0                      1820         0                 38   \n",
       "296  45.0        0                      2060         1                 60   \n",
       "297  45.0        0                      2413         0                 38   \n",
       "298  50.0        0                       196         0                 45   \n",
       "\n",
       "     high_blood_pressure  platelets  serum_creatinine  serum_sodium  sex  \\\n",
       "0                      1  265000.00               1.9           130    1   \n",
       "1                      0  263358.03               1.1           136    1   \n",
       "2                      0  162000.00               1.3           129    1   \n",
       "3                      0  210000.00               1.9           137    1   \n",
       "4                      0  327000.00               2.7           116    0   \n",
       "..                   ...        ...               ...           ...  ...   \n",
       "294                    1  155000.00               1.1           143    1   \n",
       "295                    0  270000.00               1.2           139    0   \n",
       "296                    0  742000.00               0.8           138    0   \n",
       "297                    0  140000.00               1.4           140    1   \n",
       "298                    0  395000.00               1.6           136    1   \n",
       "\n",
       "     smoking  \n",
       "0          0  \n",
       "1          0  \n",
       "2          1  \n",
       "3          0  \n",
       "4          0  \n",
       "..       ...  \n",
       "294        1  \n",
       "295        0  \n",
       "296        0  \n",
       "297        1  \n",
       "298        1  \n",
       "\n",
       "[299 rows x 11 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0833fe27-4c41-49d5-b646-9f7948efeb6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DEATH_EVENT\n",
       "0    0.67893\n",
       "1    0.32107\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ff2ea3-2069-496f-be48-fa8d5464d234",
   "metadata": {},
   "source": [
    "# Create train and test sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70ae4061-35f5-4d99-b2b6-b8514fa58a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3,stratify=y,random_state=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa268e60-41d1-401f-a14c-bffec239096d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train shape:  (209, 11)\n",
      "y train shape:  (209,)\n",
      "X test shape:  (90, 11)\n",
      "y test shape:  (90,)\n",
      "y train:  DEATH_EVENT\n",
      "0    0.679426\n",
      "1    0.320574\n",
      "Name: proportion, dtype: float64\n",
      "y test:  DEATH_EVENT\n",
      "0    0.677778\n",
      "1    0.322222\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print('X train shape: ',X_train.shape)\n",
    "print('y train shape: ',y_train.shape)\n",
    "print('X test shape: ',X_test.shape)\n",
    "print('y test shape: ',y_test.shape)\n",
    "print('y train: ',y_train.value_counts(normalize=True))\n",
    "print('y test: ',y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58c15ce-62e8-4e75-941c-26ffb1e201dc",
   "metadata": {},
   "source": [
    "# Normalize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f09675dd-16b5-4177-9ffc-5a1ff94b1870",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b494a766-faf6-4534-90d3-4354c416e470",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization \n",
    "\n",
    "## logistic Regression\n",
    "\n",
    "### Basic Approach\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28b1baa5-fe8f-4b3b-b7d7-cb67e9de9673",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/bonakdar/Desktop/xTmSM.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIPython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mImage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/Users/bonakdar/Desktop/xTmSM.png\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/P/W26-MDST-Project_Heart-Failure-Survival-Analysis/venv/lib/python3.11/site-packages/IPython/core/display.py:1025\u001b[39m, in \u001b[36mImage.__init__\u001b[39m\u001b[34m(self, data, url, filename, format, embed, width, height, retina, unconfined, metadata, alt)\u001b[39m\n\u001b[32m   1023\u001b[39m \u001b[38;5;28mself\u001b[39m.unconfined = unconfined\n\u001b[32m   1024\u001b[39m \u001b[38;5;28mself\u001b[39m.alt = alt\n\u001b[32m-> \u001b[39m\u001b[32m1025\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mImage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1026\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1028\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.width \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.metadata.get(\u001b[33m'\u001b[39m\u001b[33mwidth\u001b[39m\u001b[33m'\u001b[39m, {}):\n\u001b[32m   1029\u001b[39m     \u001b[38;5;28mself\u001b[39m.width = metadata[\u001b[33m'\u001b[39m\u001b[33mwidth\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/P/W26-MDST-Project_Heart-Failure-Survival-Analysis/venv/lib/python3.11/site-packages/IPython/core/display.py:343\u001b[39m, in \u001b[36mDisplayObject.__init__\u001b[39m\u001b[34m(self, data, url, filename, metadata)\u001b[39m\n\u001b[32m    340\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    341\u001b[39m     \u001b[38;5;28mself\u001b[39m.metadata = {}\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[38;5;28mself\u001b[39m._check_data()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/P/W26-MDST-Project_Heart-Failure-Survival-Analysis/venv/lib/python3.11/site-packages/IPython/core/display.py:1060\u001b[39m, in \u001b[36mImage.reload\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1058\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[39;00m\n\u001b[32m   1059\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.embed:\n\u001b[32m-> \u001b[39m\u001b[32m1060\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mImage\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1061\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.retina:\n\u001b[32m   1062\u001b[39m         \u001b[38;5;28mself\u001b[39m._retina_shape()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/P/W26-MDST-Project_Heart-Failure-Survival-Analysis/venv/lib/python3.11/site-packages/IPython/core/display.py:369\u001b[39m, in \u001b[36mDisplayObject.reload\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    367\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    368\u001b[39m     encoding = \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._read_flags \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m369\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_flags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    370\u001b[39m         \u001b[38;5;28mself\u001b[39m.data = f.read()\n\u001b[32m    371\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.url \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    372\u001b[39m     \u001b[38;5;66;03m# Deferred import\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/Users/bonakdar/Desktop/xTmSM.png'"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='/Users/bonakdar/Desktop/xTmSM.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbb3ab7-f518-4fe0-8120-197c8a1c461f",
   "metadata": {},
   "source": [
    "# create validation set as 10% of train set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69eac812-d035-468d-9dfa-ba8086f9ca9d",
   "metadata": {},
   "source": [
    "# Hyperparameter optimization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1161d91-b1c9-42bb-924c-5d7f658de784",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[31mInit signature:\u001b[39m\n",
       "RandomForestClassifier(\n",
       "    n_estimators=\u001b[32m100\u001b[39m,\n",
       "    *,\n",
       "    criterion=\u001b[33m'gini'\u001b[39m,\n",
       "    max_depth=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    min_samples_split=\u001b[32m2\u001b[39m,\n",
       "    min_samples_leaf=\u001b[32m1\u001b[39m,\n",
       "    min_weight_fraction_leaf=\u001b[32m0.0\u001b[39m,\n",
       "    max_features=\u001b[33m'sqrt'\u001b[39m,\n",
       "    max_leaf_nodes=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    min_impurity_decrease=\u001b[32m0.0\u001b[39m,\n",
       "    bootstrap=\u001b[38;5;28;01mTrue\u001b[39;00m,\n",
       "    oob_score=\u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       "    n_jobs=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    random_state=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    verbose=\u001b[32m0\u001b[39m,\n",
       "    warm_start=\u001b[38;5;28;01mFalse\u001b[39;00m,\n",
       "    class_weight=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    ccp_alpha=\u001b[32m0.0\u001b[39m,\n",
       "    max_samples=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "    monotonic_cst=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
       ")\n",
       "\u001b[31mDocstring:\u001b[39m     \n",
       "A random forest classifier.\n",
       "\n",
       "A random forest is a meta estimator that fits a number of decision tree\n",
       "classifiers on various sub-samples of the dataset and uses averaging to\n",
       "improve the predictive accuracy and control over-fitting.\n",
       "Trees in the forest use the best split strategy, i.e. equivalent to passing\n",
       "`splitter=\"best\"` to the underlying :class:`~sklearn.tree.DecisionTreeClassifier`.\n",
       "The sub-sample size is controlled with the `max_samples` parameter if\n",
       "`bootstrap=True` (default), otherwise the whole dataset is used to build\n",
       "each tree.\n",
       "\n",
       "For a comparison between tree-based ensemble models see the example\n",
       ":ref:`sphx_glr_auto_examples_ensemble_plot_forest_hist_grad_boosting_comparison.py`.\n",
       "\n",
       "This estimator has native support for missing values (NaNs). During training,\n",
       "the tree grower learns at each split point whether samples with missing values\n",
       "should go to the left or right child, based on the potential gain. When predicting,\n",
       "samples with missing values are assigned to the left or right child consequently.\n",
       "If no missing values were encountered for a given feature during training, then\n",
       "samples with missing values are mapped to whichever child has the most samples.\n",
       "\n",
       "Read more in the :ref:`User Guide <forest>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "n_estimators : int, default=100\n",
       "    The number of trees in the forest.\n",
       "\n",
       "    .. versionchanged:: 0.22\n",
       "       The default value of ``n_estimators`` changed from 10 to 100\n",
       "       in 0.22.\n",
       "\n",
       "criterion : {\"gini\", \"entropy\", \"log_loss\"}, default=\"gini\"\n",
       "    The function to measure the quality of a split. Supported criteria are\n",
       "    \"gini\" for the Gini impurity and \"log_loss\" and \"entropy\" both for the\n",
       "    Shannon information gain, see :ref:`tree_mathematical_formulation`.\n",
       "    Note: This parameter is tree-specific.\n",
       "\n",
       "max_depth : int, default=None\n",
       "    The maximum depth of the tree. If None, then nodes are expanded until\n",
       "    all leaves are pure or until all leaves contain less than\n",
       "    min_samples_split samples.\n",
       "\n",
       "min_samples_split : int or float, default=2\n",
       "    The minimum number of samples required to split an internal node:\n",
       "\n",
       "    - If int, then consider `min_samples_split` as the minimum number.\n",
       "    - If float, then `min_samples_split` is a fraction and\n",
       "      `ceil(min_samples_split * n_samples)` are the minimum\n",
       "      number of samples for each split.\n",
       "\n",
       "    .. versionchanged:: 0.18\n",
       "       Added float values for fractions.\n",
       "\n",
       "min_samples_leaf : int or float, default=1\n",
       "    The minimum number of samples required to be at a leaf node.\n",
       "    A split point at any depth will only be considered if it leaves at\n",
       "    least ``min_samples_leaf`` training samples in each of the left and\n",
       "    right branches.  This may have the effect of smoothing the model,\n",
       "    especially in regression.\n",
       "\n",
       "    - If int, then consider `min_samples_leaf` as the minimum number.\n",
       "    - If float, then `min_samples_leaf` is a fraction and\n",
       "      `ceil(min_samples_leaf * n_samples)` are the minimum\n",
       "      number of samples for each node.\n",
       "\n",
       "    .. versionchanged:: 0.18\n",
       "       Added float values for fractions.\n",
       "\n",
       "min_weight_fraction_leaf : float, default=0.0\n",
       "    The minimum weighted fraction of the sum total of weights (of all\n",
       "    the input samples) required to be at a leaf node. Samples have\n",
       "    equal weight when sample_weight is not provided.\n",
       "\n",
       "max_features : {\"sqrt\", \"log2\", None}, int or float, default=\"sqrt\"\n",
       "    The number of features to consider when looking for the best split:\n",
       "\n",
       "    - If int, then consider `max_features` features at each split.\n",
       "    - If float, then `max_features` is a fraction and\n",
       "      `max(1, int(max_features * n_features_in_))` features are considered at each\n",
       "      split.\n",
       "    - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
       "    - If \"log2\", then `max_features=log2(n_features)`.\n",
       "    - If None, then `max_features=n_features`.\n",
       "\n",
       "    .. versionchanged:: 1.1\n",
       "        The default of `max_features` changed from `\"auto\"` to `\"sqrt\"`.\n",
       "\n",
       "    Note: the search for a split does not stop until at least one\n",
       "    valid partition of the node samples is found, even if it requires to\n",
       "    effectively inspect more than ``max_features`` features.\n",
       "\n",
       "max_leaf_nodes : int, default=None\n",
       "    Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
       "    Best nodes are defined as relative reduction in impurity.\n",
       "    If None then unlimited number of leaf nodes.\n",
       "\n",
       "min_impurity_decrease : float, default=0.0\n",
       "    A node will be split if this split induces a decrease of the impurity\n",
       "    greater than or equal to this value.\n",
       "\n",
       "    The weighted impurity decrease equation is the following::\n",
       "\n",
       "        N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
       "                            - N_t_L / N_t * left_impurity)\n",
       "\n",
       "    where ``N`` is the total number of samples, ``N_t`` is the number of\n",
       "    samples at the current node, ``N_t_L`` is the number of samples in the\n",
       "    left child, and ``N_t_R`` is the number of samples in the right child.\n",
       "\n",
       "    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
       "    if ``sample_weight`` is passed.\n",
       "\n",
       "    .. versionadded:: 0.19\n",
       "\n",
       "bootstrap : bool, default=True\n",
       "    Whether bootstrap samples are used when building trees. If False, the\n",
       "    whole dataset is used to build each tree.\n",
       "\n",
       "oob_score : bool or callable, default=False\n",
       "    Whether to use out-of-bag samples to estimate the generalization score.\n",
       "    By default, :func:`~sklearn.metrics.accuracy_score` is used.\n",
       "    Provide a callable with signature `metric(y_true, y_pred)` to use a\n",
       "    custom metric. Only available if `bootstrap=True`.\n",
       "\n",
       "    For an illustration of out-of-bag (OOB) error estimation, see the example\n",
       "    :ref:`sphx_glr_auto_examples_ensemble_plot_ensemble_oob.py`.\n",
       "\n",
       "n_jobs : int, default=None\n",
       "    The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n",
       "    :meth:`decision_path` and :meth:`apply` are all parallelized over the\n",
       "    trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
       "    context. ``-1`` means using all processors. See :term:`Glossary\n",
       "    <n_jobs>` for more details.\n",
       "\n",
       "random_state : int, RandomState instance or None, default=None\n",
       "    Controls both the randomness of the bootstrapping of the samples used\n",
       "    when building trees (if ``bootstrap=True``) and the sampling of the\n",
       "    features to consider when looking for the best split at each node\n",
       "    (if ``max_features < n_features``).\n",
       "    See :term:`Glossary <random_state>` for details.\n",
       "\n",
       "verbose : int, default=0\n",
       "    Controls the verbosity when fitting and predicting.\n",
       "\n",
       "warm_start : bool, default=False\n",
       "    When set to ``True``, reuse the solution of the previous call to fit\n",
       "    and add more estimators to the ensemble, otherwise, just fit a whole\n",
       "    new forest. See :term:`Glossary <warm_start>` and\n",
       "    :ref:`tree_ensemble_warm_start` for details.\n",
       "\n",
       "class_weight : {\"balanced\", \"balanced_subsample\"}, dict or list of dicts,             default=None\n",
       "    Weights associated with classes in the form ``{class_label: weight}``.\n",
       "    If not given, all classes are supposed to have weight one. For\n",
       "    multi-output problems, a list of dicts can be provided in the same\n",
       "    order as the columns of y.\n",
       "\n",
       "    Note that for multioutput (including multilabel) weights should be\n",
       "    defined for each class of every column in its own dict. For example,\n",
       "    for four-class multilabel classification weights should be\n",
       "    [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n",
       "    [{1:1}, {2:5}, {3:1}, {4:1}].\n",
       "\n",
       "    The \"balanced\" mode uses the values of y to automatically adjust\n",
       "    weights inversely proportional to class frequencies in the input data\n",
       "    as ``n_samples / (n_classes * np.bincount(y))``\n",
       "\n",
       "    The \"balanced_subsample\" mode is the same as \"balanced\" except that\n",
       "    weights are computed based on the bootstrap sample for every tree\n",
       "    grown.\n",
       "\n",
       "    For multi-output, the weights of each column of y will be multiplied.\n",
       "\n",
       "    Note that these weights will be multiplied with sample_weight (passed\n",
       "    through the fit method) if sample_weight is specified.\n",
       "\n",
       "ccp_alpha : non-negative float, default=0.0\n",
       "    Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
       "    subtree with the largest cost complexity that is smaller than\n",
       "    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
       "    :ref:`minimal_cost_complexity_pruning` for details. See\n",
       "    :ref:`sphx_glr_auto_examples_tree_plot_cost_complexity_pruning.py`\n",
       "    for an example of such pruning.\n",
       "\n",
       "    .. versionadded:: 0.22\n",
       "\n",
       "max_samples : int or float, default=None\n",
       "    If bootstrap is True, the number of samples to draw from X\n",
       "    to train each base estimator.\n",
       "\n",
       "    - If None (default), then draw `X.shape[0]` samples.\n",
       "    - If int, then draw `max_samples` samples.\n",
       "    - If float, then draw `max(round(n_samples * max_samples), 1)` samples. Thus,\n",
       "      `max_samples` should be in the interval `(0.0, 1.0]`.\n",
       "\n",
       "    .. versionadded:: 0.22\n",
       "\n",
       "monotonic_cst : array-like of int of shape (n_features), default=None\n",
       "    Indicates the monotonicity constraint to enforce on each feature.\n",
       "      - 1: monotonic increase\n",
       "      - 0: no constraint\n",
       "      - -1: monotonic decrease\n",
       "\n",
       "    If monotonic_cst is None, no constraints are applied.\n",
       "\n",
       "    Monotonicity constraints are not supported for:\n",
       "      - multiclass classifications (i.e. when `n_classes > 2`),\n",
       "      - multioutput classifications (i.e. when `n_outputs_ > 1`),\n",
       "      - classifications trained on data with missing values.\n",
       "\n",
       "    The constraints hold over the probability of the positive class.\n",
       "\n",
       "    Read more in the :ref:`User Guide <monotonic_cst_gbdt>`.\n",
       "\n",
       "    .. versionadded:: 1.4\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "estimator_ : :class:`~sklearn.tree.DecisionTreeClassifier`\n",
       "    The child estimator template used to create the collection of fitted\n",
       "    sub-estimators.\n",
       "\n",
       "    .. versionadded:: 1.2\n",
       "       `base_estimator_` was renamed to `estimator_`.\n",
       "\n",
       "estimators_ : list of DecisionTreeClassifier\n",
       "    The collection of fitted sub-estimators.\n",
       "\n",
       "classes_ : ndarray of shape (n_classes,) or a list of such arrays\n",
       "    The classes labels (single output problem), or a list of arrays of\n",
       "    class labels (multi-output problem).\n",
       "\n",
       "n_classes_ : int or list\n",
       "    The number of classes (single output problem), or a list containing the\n",
       "    number of classes for each output (multi-output problem).\n",
       "\n",
       "n_features_in_ : int\n",
       "    Number of features seen during :term:`fit`.\n",
       "\n",
       "    .. versionadded:: 0.24\n",
       "\n",
       "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
       "    Names of features seen during :term:`fit`. Defined only when `X`\n",
       "    has feature names that are all strings.\n",
       "\n",
       "    .. versionadded:: 1.0\n",
       "\n",
       "n_outputs_ : int\n",
       "    The number of outputs when ``fit`` is performed.\n",
       "\n",
       "feature_importances_ : ndarray of shape (n_features,)\n",
       "    The impurity-based feature importances.\n",
       "    The higher, the more important the feature.\n",
       "    The importance of a feature is computed as the (normalized)\n",
       "    total reduction of the criterion brought by that feature.  It is also\n",
       "    known as the Gini importance.\n",
       "\n",
       "    Warning: impurity-based feature importances can be misleading for\n",
       "    high cardinality features (many unique values). See\n",
       "    :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
       "\n",
       "oob_score_ : float\n",
       "    Score of the training dataset obtained using an out-of-bag estimate.\n",
       "    This attribute exists only when ``oob_score`` is True.\n",
       "\n",
       "oob_decision_function_ : ndarray of shape (n_samples, n_classes) or             (n_samples, n_classes, n_outputs)\n",
       "    Decision function computed with out-of-bag estimate on the training\n",
       "    set. If n_estimators is small it might be possible that a data point\n",
       "    was never left out during the bootstrap. In this case,\n",
       "    `oob_decision_function_` might contain NaN. This attribute exists\n",
       "    only when ``oob_score`` is True.\n",
       "\n",
       "estimators_samples_ : list of arrays\n",
       "    The subset of drawn samples (i.e., the in-bag samples) for each base\n",
       "    estimator. Each subset is defined by an array of the indices selected.\n",
       "\n",
       "    .. versionadded:: 1.4\n",
       "\n",
       "See Also\n",
       "--------\n",
       "sklearn.tree.DecisionTreeClassifier : A decision tree classifier.\n",
       "sklearn.ensemble.ExtraTreesClassifier : Ensemble of extremely randomized\n",
       "    tree classifiers.\n",
       "sklearn.ensemble.HistGradientBoostingClassifier : A Histogram-based Gradient\n",
       "    Boosting Classification Tree, very fast for big datasets (n_samples >=\n",
       "    10_000).\n",
       "\n",
       "Notes\n",
       "-----\n",
       "The default values for the parameters controlling the size of the trees\n",
       "(e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
       "unpruned trees which can potentially be very large on some data sets. To\n",
       "reduce memory consumption, the complexity and size of the trees should be\n",
       "controlled by setting those parameter values.\n",
       "\n",
       "The features are always randomly permuted at each split. Therefore,\n",
       "the best found split may vary, even with the same training data,\n",
       "``max_features=n_features`` and ``bootstrap=False``, if the improvement\n",
       "of the criterion is identical for several splits enumerated during the\n",
       "search of the best split. To obtain a deterministic behaviour during\n",
       "fitting, ``random_state`` has to be fixed.\n",
       "\n",
       "References\n",
       "----------\n",
       ".. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from sklearn.ensemble import RandomForestClassifier\n",
       ">>> from sklearn.datasets import make_classification\n",
       ">>> X, y = make_classification(n_samples=1000, n_features=4,\n",
       "...                            n_informative=2, n_redundant=0,\n",
       "...                            random_state=0, shuffle=False)\n",
       ">>> clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
       ">>> clf.fit(X, y)\n",
       "RandomForestClassifier(...)\n",
       ">>> print(clf.predict([[0, 0, 0, 0]]))\n",
       "[1]\n",
       "\u001b[31mFile:\u001b[39m           /opt/anaconda3/envs/vmb_env_py311/lib/python3.11/site-packages/sklearn/ensemble/_forest.py\n",
       "\u001b[31mType:\u001b[39m           ABCMeta\n",
       "\u001b[31mSubclasses:\u001b[39m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RandomForestClassifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dd0342-c8e4-4414-8960-74b7a86a876f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train shape:  (188, 11)\n",
      "y train shape:  (188,)\n",
      "X test shape:  (90, 11)\n",
      "y test shape:  (90,)\n",
      "X val shape:  (21, 11)\n",
      "y val shape:  (21,)\n",
      "y train:  DEATH_EVENT\n",
      "0    0.680851\n",
      "1    0.319149\n",
      "Name: proportion, dtype: float64\n",
      "y test:  DEATH_EVENT\n",
      "0    0.677778\n",
      "1    0.322222\n",
      "Name: proportion, dtype: float64\n",
      "y val:  DEATH_EVENT\n",
      "0    0.666667\n",
      "1    0.333333\n",
      "Name: proportion, dtype: float64\n",
      "Hyperparameter tuning results:\n",
      "\n",
      "n_estimators = 20  | max_depth = 5     | Validation Accuracy = 0.8571\n",
      "n_estimators = 20  | max_depth = 10    | Validation Accuracy = 0.8095\n",
      "n_estimators = 20  | max_depth = 13    | Validation Accuracy = 0.8571\n",
      "n_estimators = 50  | max_depth = 5     | Validation Accuracy = 0.8571\n",
      "n_estimators = 50  | max_depth = 10    | Validation Accuracy = 0.9048\n",
      "n_estimators = 50  | max_depth = 13    | Validation Accuracy = 0.9048\n",
      "n_estimators = 100 | max_depth = 5     | Validation Accuracy = 0.8571\n",
      "n_estimators = 100 | max_depth = 10    | Validation Accuracy = 0.9048\n",
      "n_estimators = 100 | max_depth = 13    | Validation Accuracy = 0.8571\n",
      "n_estimators = 150 | max_depth = 5     | Validation Accuracy = 0.8571\n",
      "n_estimators = 150 | max_depth = 10    | Validation Accuracy = 0.8095\n",
      "n_estimators = 150 | max_depth = 13    | Validation Accuracy = 0.8571\n",
      "n_estimators = 200 | max_depth = 5     | Validation Accuracy = 0.8571\n",
      "n_estimators = 200 | max_depth = 10    | Validation Accuracy = 0.8095\n",
      "n_estimators = 200 | max_depth = 13    | Validation Accuracy = 0.8095\n",
      "\n",
      "Best Hyperparameters:\n",
      "{'n_estimators': 50, 'max_depth': 10}\n",
      "Best Validation Accuracy = 0.9048\n",
      "\n",
      "Final Test Accuracy = 0.3222\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "# Suppress all warnings globally\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X,y, test_size=0.3,stratify=y,random_state=21)\n",
    "# Create validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.1, random_state=21, stratify=y_train_full)\n",
    "\n",
    "print('X train shape: ',X_train.shape)\n",
    "print('y train shape: ',y_train.shape)\n",
    "print('X test shape: ',X_test.shape)\n",
    "print('y test shape: ',y_test.shape)\n",
    "print('X val shape: ',X_val.shape)\n",
    "print('y val shape: ',y_val.shape)\n",
    "print('y train: ',y_train.value_counts(normalize=True))\n",
    "print('y test: ',y_test.value_counts(normalize=True))\n",
    "print('y val: ',y_val.value_counts(normalize=True))\n",
    "\n",
    "# Normalize data based on mean and STD of train data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "n_estimators_values = [20, 50, 100, 150, 200]\n",
    "max_depth_values = [5, 10, 13]\n",
    "\n",
    "best_score = 0\n",
    "best_params = None\n",
    "\n",
    "print(\"Hyperparameter tuning results:\\n\")\n",
    "\n",
    "# 5️⃣ Nested loops\n",
    "for n_estimators in n_estimators_values:\n",
    "    for max_depth in max_depth_values:\n",
    "        \n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        y_val_pred = model.predict(X_val)\n",
    "        val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "        \n",
    "        print(f\"n_estimators = {n_estimators:<3} | \"\n",
    "              f\"max_depth = {str(max_depth):<5} | \"\n",
    "              f\"Validation Accuracy = {val_accuracy:.4f}\")\n",
    "        \n",
    "        if val_accuracy > best_score:\n",
    "            best_score = val_accuracy\n",
    "            best_params = {\n",
    "                \"n_estimators\": n_estimators,\n",
    "                \"max_depth\": max_depth\n",
    "            }\n",
    "\n",
    "# 6️⃣ Print best hyperparameters\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "print(best_params)\n",
    "print(f\"Best Validation Accuracy = {best_score:.4f}\")\n",
    "\n",
    "final_model = RandomForestClassifier(\n",
    "    n_estimators=best_params[\"n_estimators\"],\n",
    "    max_depth=best_params[\"max_depth\"],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "final_model.fit(X_train_full, y_train_full)\n",
    "\n",
    "# 8️⃣ Final Test Evaluation\n",
    "y_test_pred = final_model.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"\\nFinal Test Accuracy = {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b643c1a8-ed08-4cff-8a6a-be161191c909",
   "metadata": {},
   "source": [
    "# GridSearch CV: Python has a package that can automatically do this optimization for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120f0afa-34fd-469c-b091-9e2b8373a758",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 150 candidates, totalling 750 fits\n",
      "\n",
      "Best Hyperparameters:\n",
      "{'max_depth': 10, 'max_features': 'sqrt', 'min_samples_split': 5, 'n_estimators': 50}\n",
      "\n",
      "Best Cross-Validation Accuracy:\n",
      "0.7652729384436701\n",
      "\n",
      "Final Test Accuracy:\n",
      "0.7333333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3,stratify=y,random_state=21)\n",
    "\n",
    "#Normalize data \n",
    "# Normalize data based on mean and STD of train data\n",
    "scaler = StandardScaler()\n",
    "X_train_n = scaler.fit_transform(X_train)\n",
    "X_test_n = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "model = RandomForestClassifier(random_state=21)\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [20, 50, 100, 150, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 7, 10, 13],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid.fit(X_train_n, y_train)\n",
    "\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "print(grid.best_params_)\n",
    "\n",
    "print(\"\\nBest Cross-Validation Accuracy:\")\n",
    "print(grid.best_score_)\n",
    "\n",
    "y_test_pred = grid.predict(X_test_n)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"\\nFinal Test Accuracy:\")\n",
    "print(test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9928179a-6246-4f60-a3ed-c0b1c0657b8c",
   "metadata": {},
   "source": [
    "# Kaggle Style Competition Challenge\n",
    "\n",
    "Use GridSearch Approach on different ML classifiers and identify the best model with the highest test accuracy. You will be ranked accordingly. Top 3 students that can return the highest accuracy on test set are the winners. Keep these constant: \n",
    "\n",
    "1- X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3,stratify=y,random_state=21)\n",
    "\n",
    "2- cv=5, scoring='accuracy' in GridSearchCV\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa47998",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3zjxoi9bzrd",
   "metadata": {},
   "source": [
    "# Optuna: Modern Hyperparameter Optimization\n",
    "\n",
    "Optuna is a lightweight framework for advanced hyperparameter optimization. Unlike GridSearchCV which tests all combinations, Optuna intelligently searches the parameter space.\n",
    "\n",
    "**Key strategies:**\n",
    "- **Random Search**: Random sampling from search space\n",
    "- **Bayesian Optimization (TPE)**: Learns from previous trials to propose better parameters\n",
    "\n",
    "**Benefits:**\n",
    "- Much faster than GridSearch (especially with expensive models)\n",
    "- Handles continuous, discrete, and categorical parameters naturally\n",
    "- Can prune unpromising trials early\n",
    "- State-of-the-art for real-world problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "w3gjcrp92zb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install optuna -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "w6enjo8cbzk",
   "metadata": {},
   "source": [
    "## Random Search with Optuna\n",
    "\n",
    "Random search samples parameters uniformly at random. Simple but effective - just randomly try different combinations and pick the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qc095sh7e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.samplers import RandomSampler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=21)\n",
    "scaler = StandardScaler()\n",
    "X_train_n = scaler.fit_transform(X_train)\n",
    "X_test_n = scaler.transform(X_test)\n",
    "\n",
    "def objective_random(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 10, 200),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 30),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "        'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2'])\n",
    "    }\n",
    "    model = RandomForestClassifier(**params, random_state=21)\n",
    "    scores = cross_val_score(model, X_train_n, y_train, cv=5, scoring='accuracy')\n",
    "    return scores.mean()\n",
    "\n",
    "study_random = optuna.create_study(direction='maximize', sampler=RandomSampler(seed=21))\n",
    "study_random.optimize(objective_random, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "print(f\"Best validation accuracy: {study_random.best_value:.4f}\")\n",
    "print(f\"Best params: {study_random.best_params}\\n\")\n",
    "\n",
    "best_model_random = RandomForestClassifier(**study_random.best_params, random_state=21)\n",
    "best_model_random.fit(X_train_n, y_train)\n",
    "test_acc_random = accuracy_score(y_test, best_model_random.predict(X_test_n))\n",
    "print(f\"Test accuracy: {test_acc_random:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "x9ojehlelhp",
   "metadata": {},
   "source": [
    "## Bayesian Optimization (TPE Sampler)\n",
    "\n",
    "Bayesian Optimization is smarter - it builds a probability model of the objective function and uses it to propose promising parameter combinations. Tree-structured Parzen Estimator (TPE) is the algorithm that makes this work.\n",
    "\n",
    "The key insight: after each trial, we learn something about which parameters work better, so the next trial is more likely to be good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "h5r3304jkyv",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna.samplers import TPESampler\n",
    "\n",
    "def objective_bayesian(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 10, 200),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 30),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "        'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2'])\n",
    "    }\n",
    "    model = RandomForestClassifier(**params, random_state=21)\n",
    "    scores = cross_val_score(model, X_train_n, y_train, cv=5, scoring='accuracy')\n",
    "    return scores.mean()\n",
    "\n",
    "study_bayesian = optuna.create_study(direction='maximize', sampler=TPESampler(seed=21))\n",
    "study_bayesian.optimize(objective_bayesian, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "print(f\"Best validation accuracy: {study_bayesian.best_value:.4f}\")\n",
    "print(f\"Best params: {study_bayesian.best_params}\\n\")\n",
    "\n",
    "best_model_bayesian = RandomForestClassifier(**study_bayesian.best_params, random_state=21)\n",
    "best_model_bayesian.fit(X_train_n, y_train)\n",
    "test_acc_bayesian = accuracy_score(y_test, best_model_bayesian.predict(X_test_n))\n",
    "print(f\"Test accuracy: {test_acc_bayesian:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75p6ywmqbxb",
   "metadata": {},
   "source": [
    "## Quick Comparison\n",
    "\n",
    "|  | GridSearchCV | Random Search | Bayesian (TPE) |\n",
    "|---|---|---|---|\n",
    "| **Speed** | Slow (tests all) | Medium | Fast (learns) |\n",
    "| **# Evaluations** | All combinations | Fixed budget | Fixed budget |\n",
    "| **Best for** | Small grids | Simple & parallel | **Real projects** |\n",
    "| **Pros** | Exhaustive | Simple | Most efficient |\n",
    "| **Cons** | Expensive | No learning | Slightly complex |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gn26kvbjkhq",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "random_vals = [t.value for t in study_random.trials]\n",
    "bayesian_vals = [t.value for t in study_bayesian.trials]\n",
    "\n",
    "axes[0].plot(random_vals, 'o-', alpha=0.6)\n",
    "axes[0].axhline(y=study_random.best_value, color='r', linestyle='--', label=f'Best: {study_random.best_value:.4f}')\n",
    "axes[0].set_title('Random Search')\n",
    "axes[0].set_xlabel('Trial'), axes[0].set_ylabel('Accuracy')\n",
    "axes[0].legend(), axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(bayesian_vals, 'o-', alpha=0.6, color='green')\n",
    "axes[1].axhline(y=study_bayesian.best_value, color='r', linestyle='--', label=f'Best: {study_bayesian.best_value:.4f}')\n",
    "axes[1].set_title('Bayesian Optimization (TPE)')\n",
    "axes[1].set_xlabel('Trial'), axes[1].set_ylabel('Accuracy')\n",
    "axes[1].legend(), axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(f\"{'Method':<20} {'Val Acc':<15} {'Test Acc':<15}\")\n",
    "print(\"-\"*50)\n",
    "print(f\"{'Random Search':<20} {study_random.best_value:<15.4f} {test_acc_random:<15.4f}\")\n",
    "print(f\"{'Bayesian (TPE)':<20} {study_bayesian.best_value:<15.4f} {test_acc_bayesian:<15.4f}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ke5l232m1vm",
   "metadata": {},
   "source": [
    "## When to Use What?\n",
    "\n",
    "**GridSearchCV** → Small, well-defined parameter grid\n",
    "\n",
    "**Random Search** → Large search space, easy parallelization needed\n",
    "\n",
    "**Bayesian Optimization (Optuna)** → **Use this for most real projects!**\n",
    "- Expensive objective function (long model training)\n",
    "- Mix of continuous and discrete parameters\n",
    "- Want best results with reasonable compute\n",
    "- Working on actual problems (not toy examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ai49teof53g",
   "metadata": {},
   "source": [
    "## Bonus: Pruning (Early Stopping)\n",
    "\n",
    "If a model looks bad halfway through training, why keep going? Pruning lets Optuna stop unpromising trials early.\n",
    "\n",
    "```python\n",
    "def objective_with_pruning(trial):\n",
    "    for epoch in range(100):\n",
    "        acc = train_epoch()\n",
    "        trial.report(acc, epoch)  # Report progress\n",
    "        \n",
    "        if trial.should_prune():  # Stop if not promising\n",
    "            raise optuna.TrialPruned()\n",
    "    \n",
    "    return final_accuracy\n",
    "```\n",
    "\n",
    "This saves a lot of time, especially with deep learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55067a0e-6b93-4d0d-ad2f-195fe484fcae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
